[
  {
    "query_id": "63",
    "question": "What is the key concept for understanding NP?",
    "random_doc": {
      "doc_id": "fQvg-hh9dUw.en-qlPKC2UN_YU_14_srt",
      "content": "4 simple values, 0, 1, 2, 3. The y values are the same as x values. So this is 0,0, 1, 1, 2, 2, 3 3. They're all lying on a line. But I'm going to fit. I'm going to plot them out. And then I'm going to fit a quadratic. y if it equals ax squared plus bx plus c to this. Now I know it's a line, but I want to see what happens if I fit a quadratic. So I'm going to use polyfit to fit my quadratic. I'm going to print out some data about it. And then I'm going to use Polyval to estimate what those values should be. Plot them out. And then compute r squared value, and see what happens. All right, OK, and let me set this up better. What am I doing? I want to just fit it to a line. I know it's a line, but I'm going to fit a quadratic to it. And what I'd expect is, even though there's an extra term there, it shouldn't matter. So if I go to Python, and I run this, I run exactly that example, look at that. a equals 0, b is 1, c equals 0. Look at the r-squared value. I'll pull that together for you. It says, in this perfect case, there's what I get. The blue line is drawn through the actual values. The dotted red line is drawn through the predicted values. They exactly line up. And in fact, the solution implied says, the higher order term coefficient 0, it doesn't matter. So what it found was y equals x. I know you're totally impressed I could find a straight line. But notice what happened there. I dropped or that system said, you don't need the higher order term. Wonderful r-squared value. OK, let's see how well it predicts. Let's add in one more point, out at 20. So this is 0, 1, 2, 3. That's 0, 1, 2, 3. I'm going to add 20 in there, so it's 0, 0 , 1, 2, 2, 3, 3, 20, 20. Again, I can estimate using the same model. So I'm not recomputing the model, the model I predicted from using those first set of four points. I can get the estimated y values, plot those out, and you again, compute the r-squared value here. And even adding that point in, there's the line. And guess what. Perfectly predicts it. No big surprise. So it says, in the case of perfect data, adding the higher order terms isn't going to cause a problem. The system will say coefficients are 0. That's all I need. All right, now, let's go back and add in just a tiny bit of noise right there. 0, 0, 1, 1, 2, 2, and 3, 3.1. So I've got a slight deviation in the y value there. Again, I can plot them. I'm going to fit a quadratic to them. I'm going to print out some information about it and then get the estimated values using that new model to see what it should look like. I'm not going to run it. I'm going to show you the result.",
      "course": "6.0002",
      "lecture": "10 Understanding Experimental Data cont"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "289",
    "question": "Why is random sampling generally easier to achieve in simulations compared to in the field?",
    "random_doc": {
      "doc_id": "45e2fd621349cfd7c9faf93a6ba134a3_MIT18_404f20_lec14_8_pdf",
      "content": "Attempt to show !CFG ∈P\nTheorem: !CFG ∈P \nProof attempt:\nRecursive algorithm & tests if ' generates (, starting at any specified variable R. \n& = “On input 〈', (, R〉\n1.  For each way to divide ( = -. and for each rule R →ST\n2.       Use & to test 〈', -, S〉and 〈', ., T〉\n3.       Accept if both accept\n4.  Reject if none of the above accepted.”\nThen decide !CFG by starting from '’s start variable.\n& is a correct algorithm, but it takes non-polynomial time.\n(Each recursion makes 0(2) calls and depth is roughly log 2.) \nFix:  Use recursion + memory called Dynamic Programming (DP)\nObservation:  String ( of length 2 has 0(27) substrings (8 ⋯(:\ntherefore there are only 0(27) possible sub-problems 〈', -, S〉to solve.  \nS\nT\nR\n(\n-\n.\n8\n",
      "course": "18.404J",
      "lecture": "Lecture 14 - P and NP, SAT, Poly‑Time Reducibility"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "93",
    "question": "What does the space hierarchy theorem state about space-bound functions?",
    "random_doc": {
      "doc_id": "5af20311b02eaab959fcdb7ffb5694d3_MIT6_0002F16_lec6_3_pdf",
      "content": " \n \n \n \n \n \n \n \n \n \n  \n \nA Little History\nUlam, recovering from an illness, was playing a lot of\nsolitaire\nTried to figure out probability of winning, and failed\nThought about playing lots of hands and counting\nnumber of wins, but decided it would take years\nAsked Von Neumann if he could build a program to\nsimulate many hands on ENIAC\n6.0002 LECTURE 6 \n3\nImage of ENIAC programmers © unknown.This content\nis excluded from our Creative Commons license. For more\ninformation,see https://ocw.mit.edu/help/faq-fair-use/.\n",
      "course": "6.0002",
      "lecture": "Lecture 6: Monte Carlo Simulation"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "196",
    "question": "What is the base case when dealing with an empty suffix in dynamic programming problems?",
    "random_doc": {
      "doc_id": "19a63b4aaa3fd9c75cd1b8e940654f53_MIT6_0002F16_lec13_4_pdf",
      "content": " \n \n \nAn Example (similar to earlier lecture) \nFeatures \nLabel \nName \nEgg-laying Scales \nPoisonous \nCold­\nblooded \nNumber \nlegs \nReptile \nCobra \n1 \n1 \n1 \n1 \n0 \n1 \nRattlesnake 1 \n1 \n1 \n1 \n0 \n1 \nBoa \n0 \n1 \n0 \n1 \n0 \n1 \nconstrictor \nChicken \n1 \n1 \n0 \n1 \n2 \n0 \nGuppy \n0 \n1 \n0 \n0 \n0 \n0 \nDart frog \n1 \n0 \n1 \n0 \n4 \n0 \nZebra \n0 \n0 \n0 \n0 \n4 \n0 \nPython \n1 \n1 \n0 \n1 \n0 \n1 \nAlligator \n1 \n1 \n0 \n1 \n4 \n1 \n6.0002 LECTURE 13 \n4 \n",
      "course": "6.0002",
      "lecture": "Lecture 13: Classification"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "291",
    "question": "How does an adjacency list represent the edges of a node?",
    "random_doc": {
      "doc_id": "88f789664d3236a64481714fc911d119_MIT18_404f20_lec18_1_pdf",
      "content": " \n \n  \n \n \n \n \n \n  \n \n \n  \n \n \n \n \n \n  \n \n \n \n \n \n18.404/6.840 Lecture 18 \nLast time: \n- Space complexity \n- SPACE ! \" , NSPACE ! \" , PSPACE, NPSPACE \n- Relationship with TIME classes \nToday: (Sipser §8.3) \n- Review $%&&'(DFA ∈ PSPACE \n- Savitch’s Theorem: NSPACE ! \" \n⊆ SPACE !. \" \n- PSPACE-completeness \n- /012 is PSPACE-complete \n1 \n",
      "course": "18.404J",
      "lecture": "Lecture 18 - PSPACE‑Completeness"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "151",
    "question": "How does memoization change the time complexity of computing Fibonacci numbers?",
    "random_doc": {
      "doc_id": "367ebcbfde99ec18e14e87b69f564035_MIT6_0002F16_lec12_6_pdf",
      "content": " \n \n \n \n \n \n \n  \n \n  \n \n  \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nHiearchical Clustering \n1. Start by assigning each item to a cluster,  so that if \nyou have N items,  you now have N clusters,  each \ncontaining just one item. \n2. Find the closest (most similar) pair  of clusters and \nmerge them into a single cluster,  so that now you have \none fewer  cluster. \n3. Continue the process until all items are clustered  \ninto a single cluster  of size N.  \nWhat does distance mean?  \n6.0002  LECTURE 12 \n6 \n",
      "course": "6.0002",
      "lecture": "Lecture 12: Clustering"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "75",
    "question": "What is one of the conditions of the Pumping Lemma for context-free languages (CFLs) concerning the substring x of a string s?",
    "random_doc": {
      "doc_id": "4nXw-f6NJ9s.en-j3PyPqV-e1s_8_srt",
      "content": "which is the introduction to computer graphics course. In fact, my background was working in an animation studio for a little bit of time, and got one movie credit out of it until they changed the standards for movie credits, and then that stopped happening. But in any event, if you watch-- what's that movie-- Up, with the old man. If you hit pause at just the right moment, you can find me right above the list of babies that were born during production. But in any event-- although computer graphics might not sound like an algorithmic discipline, I'll try to convince you guys that, in some sense, you could take just about anybody in our department, have them teach 6.006, and give a similar talk that, like, the material that you've encountered in this course is going to be relevant to your life. The other course that I teach that might be of interest-- and actually, is a little more theoretically flavored-- that I teach is 6.838. So since Erik so kindly put my name on the board here, I guess I can draw The So the main object of interest in 6.838 is a particular thing called the simplicial complex. Usually, in 6.006, we spend a lot of time thinking about graphs. Let me draw you a graph. So I'm going to take a square and subdivide it. And now, let's say I put edges diagonally like that. Now, in 6.006, this thing is just a bunch of nodes connected by edges. In fact, if I took this edge and I moved it down or something, it would be the same graph. But of course, in a lot of computer graphics applications, this thing also looks an awful lot like a square. And the reason is that, of course, the graph here contains triangles inside of it. And so for instance, maybe I think of my graph as a collection of vertices, a collection of edges. This is the sort of notation we've seen before. And then I add a third thing to my description, which is a set of triplets. That's a set of triangles here. And we can take a lot of the algorithms that we've talked about in this class and extend it to this case. For example, here's a deceptively annoying one. Let's say that I want the shortest path between two vertices of my graph. We certainly have learned Dijkstra's algorithm. That's one technique to do that. And indeed, common practice in computer graphics, which is shameful, is on your triangle mesh, if you want the shortest path between two vertices, run Dijkstra's algorithm on the edges. And let's see if that works really quick. Let's say that I want the shortest path between-- and, by the way, I'm going to assume the length of my edges are the lengths as I've drawn them on the board here. So it's like 1, 1, square root of 2. OK. So let's say I want the shortest path between the bottom left and the upper right. If I run Dijkstra's algorithm, we're in good shape, right? We get-- I'll let you do the computations at home. You'll get the path that is these two edges. But here's a really annoying thing. Let's say, instead, I wanted the shortest path from the upper left to the lower right. If I run Dijkstra's algorithm on this triangulated square, what's going to be the shortest path? Yeah. In fact, there's a bunch of them. One of them might go all the way down, and then all the way to the right. What's the length of this path? 1, 2, 3, 4. Is that the length of the shortest path? Well, probably not. Well, we would like our shortest path to do something like that. But graphs don't know how to talk to triangles. And this is going to be a problem. In fact, it wasn't until fairly recently [INAUDIBLE] history terms that we were able to kind of work out the correct algorithm for the shortest path in a triangulated domain like this. And that's the runtime that we would expect. This is called MMP. I'm guessing Erik and Jason could do a better job describing it than I can. But the basic idea of the MMP algorithm actually is a really-- happens to be a nice extension of the way that we taught Dijkstra's algorithm in 6.006, because they really do keep track of these level sets of the distance function. But now, the level sets have to-- oops-- have to window and edge like that when I compute shortest path, which is a giant headache. This is one of these algorithms that was known in theory about 10 years before anybody bothered to implement it in a way that they could convince themselves it ran in n log n time. And nowadays, there's a cottage industry in computer graphics research papers to implement this and then speed it up in different ways. And sadly, the reality is that a different algorithm that we cover in 6.838 called fast marching-- which doesn't actually give you the shortest path, but some approximation thereof-- is faster, easier to use, and basically indistinguishable. In any event, in 6.838, we kind of have an interesting dual-mindset. We'll talk about a lot of algorithms that look like what we've done in whatever this class is-- 6.006. But at the same time, start to have a more geometric flavor, and we don't worry quite as much about [INAUDIBLE].. So in our computation model, oftentimes, we're kind of OK with real numbers, because that's not where the headache is. And of course, when you write code in this class, you use double-precision floating-point. If you're more responsible, like in Jason's previous lecture, you should probably keep track of the number of operations to make sure that your error is counted. But I'm not sure that we really bother with that. In any event, this allows us to have two different mindsets. There's one mindset, which is discrete. There's another mindset, which is smooth. We think about understanding geometry, like these triangular domains, as an approximation of a smooth surface. And then we might want to do stuff like compute curvature and so on, which is really associated with computing derivatives, which of course, we'll have on these kinds of simplicial objects. And that leads to this really fun area of math and computer science, whatever, called discrete differential geometry, which sounds like a contradiction in terms. And it's something that we covered in quite some detail in this course. So we build up, all of calculus, that the only calculations you're left to do are on the vertices and edges and triangles of a triangle mesh. And get pretty far, including some constructions of topology, like the Duran complex, and so on. I would argue, actually, if you take our course and then the differential geometry courses in that department, somehow, some of the indices and headaches that you often encounter in that world are much more concrete when you try to make them work on a mesh. In any event, I think I've already spent all of my time.",
      "course": "6.006",
      "lecture": "21 AlgorithmsNext Steps"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "16",
    "question": "What arguments are made to support using a one-tape Turing machine as a model for defining time complexity classes?",
    "random_doc": {
      "doc_id": "IycOPFmEQk8.en-j3PyPqV-e1s_15_srt",
      "content": "Now let's look at what the transition function, how that operates. So the transition function, remember, tells how the machine is actually doing its computation. And it says that, if you're in a certain state and the head is looking at a certain tape symbol, then you can go to a new state. You write a new symbol at that location on the tape. And you can move the head either left or right. So that's how we get the effect of the head being able to be bi-directional. And here is the writing on the tape. It comes up right here. So just an example here which says that, if we're in state two and the head is looking at an a currently on the tape, then we can move the state r. We change that a to a b. And we move the head right 1 square. Now, this is important. When you give a certain input here to the Turing machine, it may compute around for a while, moving its head back and forth, as we were showing. And it may eventually halt by either entering the q accept state or the q reject state, which I didn't bring out here, but that's important. These are the accepting, rejecting, special states of the machine. Or the machine may never enter one of those. It may just go on, and on, and on and never halt. We call that looping, a little bit of a misnomer, because looping implies some sort of a repetition. For us, looping just means not halting. And so therefore, M has three possible outcomes for each input, this w. It might accept w by entering the accept state. It could reject w by entering the reject state, which means it's going to reject it by halting. Or we also say we can reject by looping. You can reject the string by running forever. That's just the terminology that's common in the subject. So you either accept it by halting and accepting or rejecting it by either halting and rejecting or by just going forever. That's also considered to be rejecting, sort of rejecting in a sense by default. If you never actually have accepted it, then it's going to be rejected. OK, check in three here-- all right, so now our last check in for the day, we say, this Turing machine model is deterministic. I'm just saying that. But if you look at the way we set it up, if you've been following the formal definition so far, you would understand why it's deterministic. So let's just, as a way of checking that, how would we change this definition? Because we will look at the next lecture at non-deterministic Turing machines. So a little bit of a lead in to that, how would we change this definition to make it a non-deterministic Turing machine? Which of those three options would we use? So here, I'll launch that poll. I've got about 10 people left. Let's give them another 10 seconds. OK, I think that's everybody who has answered it from before. So here, I think you pretty much almost all of you got the right idea. It is B, in fact, because when we have the power set symbol here, that means there might be several-- there is a subset of possibilities. So that indicates several different ways to go. And that's the essence of non-determinism. OK, so I think we're-- whoops. All right, so look, this is also kind of setting us up for next lecture and where we're going to be going with this. So these are basically two in a-- well, two or three important definitions here. One is-- we talked about the regular languages from finite automata. We talked about the context-free languages from the grammars and the pushdown automata. What are the languages that the Turing machines can do? Those are called, in this course, anyway, Turing-recognizable languages, or T recognizable. Those are the languages that the Turing machine can recognize. And so just to make sure we were on the same page on this, the language of the machine is the collection of strings that the machine accepts. So the things that are not in the language are the things that are rejected either by looping or by halting and rejecting. So only the ones that are accepted is the language. Every machine has just a single language. It's the language of all strings that that machine accepts. And we'll say that and recognize that language, if that language is the collection of such strings that are accepted. And we will call that language a Turing-recognizable language, if there is some Turing machine that can recognize it. Now, this feature of being able to reject by running forever is a little bit weird, perhaps. And from the standpoint of practicality, it's more convenient if the machine always makes a decision to accept or reject in finite time and doesn't just reject by going forever. And so we're going to bring out a special class of Turing machines that have that feature, that they always halt. The halting states, by the way-- maybe it didn't say this explicitly-- are the q accept and the q reject states. The accept and reject states are the halting states. So if the machine halts, that means it ends up in one of those two. So it has made a decision of accepting or rejecting at the point at which it has halted. So we'll say a machine is a decider if it always halts on every input. So for every w fed in, the machine is eventually going to come to a q accept or a q reject. We call such a machine a decider. And now we're going to say, a language is-- so we'll say that the machine decides a language if it's the language of the machine, so the collection of accepted strings, and the machine is the decider. We'll say that, instead of just recognizing the language, we'll say that it decides the language. And the Turing-decidable language is a language that the machine-- of all strings the machine accepts for some Turing machine which is a decider, which is a Turing machine that always halts. So if a Turing machine may sometimes reject by looping, then it's only recognizing its language. If the Turing machine is always halting, so it's always rejecting by explicitly coming to a reject state and halting, then we'll say it's actually deciding the language. So then, in a sense, that's better. And we're going to distinguish between those two, because they're not the same. There are some languages which can be recognized, but not decided. And so in fact, we're going to get the following picture here, that the Turing-recognizable languages are a proper subset. They include all of-- everything that's decidable, certainly is going to be recognizable, because being a decider is an additional restriction to impose, an additional requirement. So everything that's decidable is going to be automatically recognizable. But there are things which are recognizable which are not decidable, as we'll see. I'll actually give an example of that, but not prove it next lecture. And just for, just to complete out this picture, I'm going to also point out-- we haven't proven this yet, but we will prove it-- that the decidable languages also include all the context-free languages, which, in turn, include the regular languages, as was already seen. So we haven't shown this inclusion yet. But actually, this is the picture that we get. So there is actually a hierarchy of containments here. Regular languages are a subset of the context-free languages, which are, in turn, a subset of the decidable languages, which in turn, are a subset of the Turing-recognizable languages.",
      "course": "18.404J",
      "lecture": "5 CF Pumping Lemma Turing Machines"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "37",
    "question": "What does it mean for a set to be T-recognizable in the context of showing that ! is T-recognizable iff there is a decidable set related to a collection of string pairs?",
    "random_doc": {
      "doc_id": "IycOPFmEQk8.en-j3PyPqV-e1s_11_srt",
      "content": "Often, the challenge in applying the pumping lemma in either case that we've seen involves choosing that string that you need to pump, that you're going to pump. So you have to choose s in F, which is longer than p, which s to go with. So you might try this one, first glance. Here is a string that's in the language, because it's two copies of the string 0 to the p1 0 to-- and then 0 to the p1. So that's in the language, but it's a bad choice. Before I get ahead of myself, let's draw a picture of s, which I think is always helpful to see. So here is runs of 0's and then a 1, runs of 0's and then a 1. Why is this a bad choice? Because you can pump that string and you remain in the language. There is a way to cut that string up and you'll stay in the language. And the way to cut it up is to let the x be just that substring which is just the 1. And the v and y can be a couple of 0's or a single 0 on either side of that 1. And now that's going to be a small vxy. But if you repeat v and y, you're going to stay in the language, because you'll just be adding 0's here. You'll be adding same number of 0's there. And then you're going to have a string which still looks like ww. And you'll still be in the language. So that means that cutting it up doesn't get you out of the language under pumping. And the fact is that that's a bad choice for s, because there is that way of cutting it up. So you have to show there's no way-- you don't get to pick the way to cut it up. You have to show that there is no way to cut it up in order to violate the pumping lemma. So if instead you use the string 0 to the p, 1 to the p, 0 to the p, 1 to the p-- so this is 0's followed by 1's followed by 0's followed by 1's, all the same number of them-- that can't be pumped satisfying the three conditions. And just going through that-- now if you try to break it up, you're going to lose. Or the lemma is going to lose. You're going to be happy, but the lemma is not going to be happy, because it's not going-- it's going to violate the condition. Condition three says vxy is not-- doesn't span too much, and in fact, can't span two runs of 0's or two runs of 1's. It's just not big enough, because they're more than p things-- they're p things apart. And this one string, this string vxy is only p long. And so therefore, if you repeat v and y, you're going to have two runs of 0's or two 1's that have unequal length. And now that's not going to be the form ww. You're going to be out of the language. So I hope that's-- you've got a little practice with that. I think we're at our break. And I will see you back here in five minutes, if I can get my timer launched here. OK, so see you soon. This is a good time, by the way, to message me or the TAs. And I'll try to be looking for if you have any questions. In the pumping lemma, can x-- yeah, x can be epsilon in the pumping lemma. x can be epsilon. y can be epsilon, but x and y cannot both be epsilon, because then, when you pump, you'll get nothing new. Technically, v and y can include both 0's and 1's. Yeah, v and y can include both 0's and 1's. So let me try to put that back, if that's will-- so v and y can have both 0's and 1's, but they can't have 0's from two different blocks. And you can't have 1's from two different blocks. So what's going to happen is either you're going to get things out of order when you repeat-- like, a v has both 0's and 1's in it. When you repeat v, you're going to have 0's and 1's, and 0's and 1's, and 0's and 1's. That's clearly out of the language, so that's no good. Your only hope is to have v to be sticking only inside the 0's and y to be sticking only inside 0's or only inside 1's. But now, if you repeat that and just look at what you're going to get, you're going to have a string which is going to be-- if you try to cut that string in half, it's not going to be of the right form. It's not going to be two copies of the same string, because it's going to have a run of 0's followed by a longer or shorter run of 0's, or a run of 1's followed by another run of 1's of unequal length. So there is no way this can be two strings, two copies of the same string, because that's what you required. F has to be two copies of the same string to be in the language. OK, let me just see where-- we're running out of time here. Let me just put my timer here. We've only got 30 seconds. And I'm sorry I'm not getting to answer all the questions here. OK, we are done with our break. It's going to come back. And now we're shifting gears in a major way, because in a sense, everything we've",
      "course": "18.404J",
      "lecture": "5 CF Pumping Lemma Turing Machines"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "188",
    "question": "How can we use a path tree to solve the reachability problem in a graph?",
    "random_doc": {
      "doc_id": "aa05d858752700adcf734b7cdb7a699f_MIT6_0002F16_lec10_51_pdf",
      "content": "MIT OpenCourseWare\nhttps://ocw.mit.edu\n6.0002 Introduction to Computational Thinking and Data Science\nFall 2016\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms.\n",
      "course": "6.0002",
      "lecture": "Lecture 10: Understanding Experimental Data (cont.)"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "146",
    "question": "What is the formula for calculating the minimum total difficulty from note t_i with starting finger f?",
    "random_doc": {
      "doc_id": "TSI3LR5WZmo.en_2_srt",
      "content": "okay so in in interactive proofs there are two parties um and i'm going to think about them as one of them is going to be the professor okay so the professor is going to play the role of the verifier in a sense but it's like that the one who checks um and uh the professor being kind of old and tired he's been teaching too long maybe can only operate in probabilistic polynomial time so the professor if wants to tell whether two graphs are isomorphic or not probabilistic polynomial time doesn't seem to be enough to tell whether two graphs are isomorphic or not because it seems to be a more than polynomial problem however the professor has um help it has an army of graduate students and the graduate students they're not limited uh in the same way the professor is the graduate students are young they are energetic they can stay up all night they know how to code so the graduate students have unlimited computational ability so that we're going to think of the graduate students playing the role of the approver because they're not they're not limited in their capabilities we'll assume the professor on the other hand is limited so the professor wants to know if the two graphs are isomorphic let's say whatever they are um can't do it by himself so he's going to ask his students to figure out the answer and report back now there's only one problem the professor knows that students uh well in the old days they'd like to party i guess these days they like to play on uh play computer games a lot and so they're not really that eager to spend all their time figuring out whether graphs are isomorphic so he's worried that the the students will just come up with some answer and figure that he won't be able to tell the difference so the professor does not trust the students it's not enough to he for the professor to give the problem to the students and just take any answer that they're going to give the professor wants wants to be convinced okay so um now how could the students convince the professor of the answer that they've really done the work and figured out whether the graphs are isomorphic or not well if the graphs are isomorphic if it turns out that the graphs were isomorphic and the students figure that out then life is good because what are they going to do to convince the professor they're going to hand over the isomorphism and show yeah i mean they are you know those graphs really are isomorphic and here's how the correspondence works professor can check oh yeah i i now not now i'm convinced but suppose the graphs were not isomorphic what are we going to do then um the students have figured out where after night else the professor wants wants to be convinced oh no what are we going to do well in fact we're going to engage the the professor and the students are going to engage in the following protocol dialogue what's going to happen is now you have to make sure you're you're this is critical to follow to understand this little part of the story here because it's really going to set the pattern for everything in today's and tomorrow and to today's lecture and the and the next lecture okay so we're going to engage in a following interaction between the students and the professor which is going to enable the students to convince the professor that the two graphs really are not isomorphic so how is that going to work this is a beautiful little uh thing by the way so the professor is going to take the two graphs and pick one of them at random because the two graphs g and h um let's say they're not they really are not isomorphic the professor doesn't know that for sure that's what the students claim the professor really wants to not be convinced that the students are right um so the professor's gonna pick one of the two at random randomly permute that uh that choice the one that he picked and hand it over to the students say okay here is one of those two graphs randomly scrambled then i'm going to ask the students which one did i pick okay now if the graphs were really not isomorphic the students can check whether that randomly scrambled graph is isomorphic to either g or to h it's going to be isomorphic to one or the other and then they students can figure it out and they say oh you picked g or no you picked h as the case may be the students can figure that out but if the graphs were isomorphic then that scrambled version of g or h could equally well have come from either of them and the students would have no way of knowing which one the professor picked so that there's nothing they could do which would be better than guessing so if we do that a bunch of times the professor picks at random sometimes secretly of course the picks the grip picks either g or picks h and the students get it right every time either the students are really doing the work and the graphs are really not isomorphic or the students are just incredibly lucky they're managing to guess right let's say a hundred times so how the the the professor randomly and secretly picks grh uses this uses its probabilism flips a coin just a two-sided coin and says okay sometimes if we're going to do g sometimes they're going to do h just completely at random picks one or the other and then with some more randomness gets finds a random permutation of the one that he picked and then sends that over to the students and say which one did it come from um i'm not sure okay so let's pause here let's let's make sure we all understand this because this is really important um so i'm getting a question here how do we i'm not sure what your question is um okay so let me just say yeah the professor's going to play the role the verifier the graduate students play they're all approver that's coming but i really want to understand this protocol here okay so how is the professor picking the graph skin if you're okay i don't you know picking the graphs at random you have just two graphs they're in part of the input uh the both the students and the professor can see the graphs and the professors are just picking one of them at random using a coin so i'm not sure i understand the question there could p and v engage in a protocol where the secretary is on the prover side instead the question of revealing the isomorphism i there is no why so i'm not sure i understand this question either um maybe we'll make this clear you know if for for this little illustration the professor doesn't know the graphs could be isomorphic or they could be not isomorphic and so uh the professor wants to be convinced either way whatever the students whatever answer the students come up with we're going to shift this into a problem about a um deciding a language next but right now i'm just trying to give a sense of the how the model works i want to move from this informal model and now i'm going to formalize that in terms of model which will be deciding a language okay so so the interactive proof system model we have two interacting parties a verifier which is probabilistic polynomial time playing played by the professor in the previous slide and the prover which is unlimited computational power played by the students in the previous slide both of them get to see the input which in the previous case well it could be for example the pair of graphs they exchange a polynomial number of polynomial size messages so the whole exchange including the verifier's own computation is going to be polynomial the only thing that's not not not included within the computational cost is the prover's work which is unlimited um after that the verifier after the interaction the verifier will accept or reject and we're going to define the probability that the verifier together with a particular approver ends up accepting as you look over the different possible coin tosses of the verifier which could lead to different behavior on the part of the verifier and therefore different behavior on the part of the approver so over all the different possibility possibilities for the verifiers computation we're going to look at the probability that the verifier with this particular approver ends up accepting and i've written it this way this is the probability of the verifier interacting with the prover accepts the input is just simply that um and so we're going to work through an example we're going to work through the previous example more precisely in a second the class ip for interactive proofs stands for it's the class of languages such that for some verifier and approver um for strings in the language the prover makes the verifier accept with high probability and here's the interesting part for strings not in the language the prover makes it except with low probability but every there's no prover which can make it except with high probability so there's no way to cheat if you think about it in the case of the graphic non-isomorphism there's nothing you know if if the graphs were really isomorphic and the students were trying to in a devious way prove through that protocol that they're not isomorphic they would fail because there's nothing they can do if the graphs were isomorphic then um when the verifier the the professor picks one or the other at random um and scrambles it the students would have no way of telling which one the professor did so no matter what kind of scheme they try to come up with they're going to be out of luck so it's no mat for any strategy for strings that are not in the language for any s any prover calling that p with a tilde to stand for a devious or crooked prover for any uh possibly crooked prover even that with working with the verifier is still going to end up accepting with low probability so strings in the language there's going to be an honest prover who just follows the protocol in the correct way which makes the verifier accept with high probability for strings not in the language every prover is going to fail to make it accept with high probability um okay so that i mean the way i like to think about it is that p tilde is a possibly crooked proverb which is trying to make the verifier accept when it shouldn't because the string is not in the language it's like you know it's like even you can think of this in the case of um satisfiability um you know you a crooked prover might try to convince of the verifier that the formula is satisfiable when it isn't by by somehow trying to produce a satisfying assignment but that's going to be impossible there's nothing any strategy can possibly work when the formula is not satisfiable if that's what the verifier is going to check it's going to be looking for that satisfying assignment okay and by the way this is we're not going to prove this but it's really going to be proved in the same way you can make that one third error that could that occurs here something very tiny by the same kind of repetition argument okay so let's see um so why can't the prover in the first case be crooked um the prover in the first case would could be crooked but that's not going to serve the purposes um you know what what we want to show um you think about it like we think about np for strings in the language there exists a certificate there is a proof that you're in the language so if somebody is going to not produce the proof that's irrelevant the question is if you look at the best possible case the best possible prover um you know who's going to be able we're asking does there exist a way to convince the verifier that the um string is in the language so it doesn't matter that there might be some other uh silly way that doesn't work we're just looking at the best possible way so the best possible way when you're in the language is going to end up with a verifier having high probability when you're not in the language the best possible way is still going to end up with low probability when when i talk about best possible i'm trying to maximize the probability that the verifier is going to end up accepting let's continue um not sure as clear as i would like but um maybe again we're going to we're going to stick with that example because this is a very uh helpful example and to try to understand the setup and uh so we're gonna i'm gonna revisit that previous example about non-isomorphism but now in the context of this thinking about as a language so we're going to take this non-isomorphism um uh yeah we're going to take the non-isomorphism problem and show that it's an ip so there's going to be a verifier together with approver which are going to make the verifier accept with high probability for strings in the language namely graphs not ice being isomorphic and nothing there's going to be no way to make the verifier except with high probability for strings out of the language therefore that's when the graphs are isomorphic okay um so the protocol is just gonna we're gonna repeat the following thing twice you know i said in the previous case do it a hundred times just to help us think about it but actually",
      "course": "18.404J",
      "lecture": "25 Interactive Proof Systems IP"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "85",
    "question": "What is the opinion about the possibility of proving P = NP or P ≠ NP within 20 years?",
    "random_doc": {
      "doc_id": "1VhnDdQsELo.en-j3PyPqV-e1s_8_srt",
      "content": "If you compare these two classes, P and NP. P, first of all, is going to be a subset of NP, both in terms of the way we defined it because, deterministic machines are a special case of non-deterministic machines. But also if you want to think about testing membership, if you can test membership easily then you can certainly verify it in terms of the certificate. You don't even need it. The certificate is irrelevant at that point. Because whatever the certificate is, you can still test yourself whether the input is in the language or not. The big question, as I mentioned, is whether these two classes are the same. So does being able to verify membership quickly, say with one of these certificates, allow you to dispense with the certificate? Not even need a certificate and just test it for yourself whether you're in the language. And do that in polynomial time. That's the question. For a problem like Hamiltonian path, do you need to search for the answer if you're doing it deterministically? Or can you somehow avoid that and just come up with the answer directly with a polynomial time solution? Nobody knows the answer to that. And it goes back, at this point, quite a long time. It's almost 60 years now. That problem has been around 60 years. No, that would be 50 years. No, 50 years, almost 50 years. Most people believe that P is different from NP. In other words, that there are problems in P-- in NP which are not in P. A candidate would be the Hamiltonian path problem. But it seems to be very hard to prove that. And part of the reason is, how do you prove that a problem like Hamiltonian path does not have a polynomial time algorithm. It's very tricky to do that, because the class of polynomial time algorithms is a very rich class. Polynomial time algorithms are very powerful. And to try to prove-- there's no clever way of solving the Hamiltonian path problem. It just seems to be beyond our present day mathematics. I believe someday somebody's going to solve it. But so far, no one has succeeded. So what I thought we would do is-- I think I have a check-in here.",
      "course": "18.404J",
      "lecture": "14 P and NP SAT Poly-Time Reducibility"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "77",
    "question": "What does the GNFA conversion process involve with respect to the states?",
    "random_doc": {
      "doc_id": "a77711ed3d212bf472f3485883a121e0_MIT18_404f20_lec3_7_pdf",
      "content": "is regular → every long s \nis also \n \n \n \n \n \n \n   \n \n \n \n \n  \n \n \n  \n  \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n  \n  \n  \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \n   \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \n   \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntring in ! can be pumped and the result stays in !.\nbe the number of states in 0. Pick # ∈! where # ≥\".\n'\n(\n)\n0\n12\n#\nThe path that 0 foll\nwhen reading #.\naccepted\nMethod for Proving Non-regularity \nPumping Lemma:   For every regular language !, \nthere is a number \" (the “pumping length”) such that \nif # ∈! and # ≥ \" then # = '() where \n1) '(*) ∈ ! for all + ≥0\n(* = (( ⋯( \n2) ( ≠ ε\n'( ≤ \"\n3) \n+\n}\nInformally: ! \nProof:  Let DFA 0 recognize !. Let \" \n'\n( \n)\n# = \n12\n12\n0 will repeat a state 12 when reading \nbecause # is so long. \n'\n(\n( \n) \n12\n12\n12\nCheck-in 3.2 \nThe Pumping Lemma depends on the fact that \nif 0 has \" states and it runs for more than \" steps \nthen 0 will enter some state at least twice. \nWe call that fact: \n(a) The Pigeonhole Principle\n(b) Burnside's Counting Theorem\n(c) The Coronavirus Calculation\nCheck-in 3.2 \n7 \n",
      "course": "18.404J",
      "lecture": "Lecture 3 - Regular Pumping Lemma, Finite Automata → Regular Expressions, CFGs"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "321",
    "question": "What do the varying expected returns from 100 spins and 1,000,000 spins of Fair Roulette indicate about the nature of Monte Carlo simulations?",
    "random_doc": {
      "doc_id": "C1lhuz6pZC0.en-qlPKC2UN_YU_5_srt",
      "content": "I called it a flexible greedy primarily because of this key function over here. So you'll notice in red there's a parameter called keyfunction. That's going to be-- map the elements of items to numbers. So it will be used to sort the items. So I want to sort them from best to worst, and this function will be used to tell me what I mean by best. So maybe keyfunction will just return the value or maybe it will return the weight or maybe it will return some function of the density. But the idea here is I want to use one greedy algorithm independently of my definition of best. So I use keyfunction to define what I mean by best. So I'm going to come in. I'm going to sort it from best to worst. And then for i in range len of items sub copy-- I'm being good. I've copied it. That's why you sorted rather than sort. I don't want to have a side effect in the parameter. In general, it's not good hygiene to do that. And so for-- I'll go through it in order from best to worst. And if the value is less than the maximum cost, if putting it in would keep me under the cost or not over the cost, I put it in, and I just do that until I can't put anything else in. So I might skip a few because I might get to the point where there's only a few calories left, and the next best item is over that budget but maybe further down I'll find one that is not over it and put it in. That's why I can't exit as soon as I reach-- as soon as I find an item that won't fit. And then I'll just return. Does this make sense? Does anyone have any doubts about whether this algorithm actually works? I hope not because I think it does work. Let's ask the next question. How efficient do we think it is? What is the efficiency of this algorithm? Let's see where the time goes. That's the algorithm we just looked at. So I deleted the comment, so we'd have a little more room in the slide. Who wants to make a guess? By the way, this is the question. So please go answer the questions. We'll see how people do. But we can think about it as well together. Well, let's see where the time goes. The first thing is at the sort. So I'm going to sort all the items. And we heard from Professor Grimson how long the sort takes. See who remembers. Python uses something called timsort, which is a variant of something called quicksort, which has the same worst-case complexity as merge sort. And so we know that is n log n where n in this case would be the len of items. So we know we have that. Then we have a loop. How many times do we go through this loop? Well, we go through the loop n times, once for each item because we do end up looking at every item. And if we know that, what's the order? AUDIENCE: [INAUDIBLE]. JOHN GUTTAG: N log n plus n-- I guess is order n log n, right? So it's pretty efficient. And we can do this for big numbers like a million. Log of a million times a million is not a very big number. So it's very efficient. Here's some code that uses greedy. Takes in the items, the constraint, in this case will be the weight, and just calls greedy, but with the keyfunction and prints what we have. So we're going to test greedy. I actually think I used 750 in the code, but we can use 800. It doesn't matter. And here's something we haven't seen before. So used greedy by value to allocate and calls testGreedy with food, maxUnits and Food.getValue. Notice it's passing the function. That's why it's not-- no closed parentheses after it. Used greedy to allocate. And then we have something pretty interesting. What's going on with this lambda? So here we're going to be using greedy by density to allocate-- actually, sorry, this is greedy by cost. And you'll notice what we're doing is-- we don't want to pass in the cost, right, because we really want the opposite of the cost. We want to reverse the sort because we want the cheaper items to get chosen first. The ones that have fewer calories, not the ones that have more calories. As it happens, when I define cost, I defined it in the obvious way, the total number of calories. So I could have gone and written another function to do it, but since it was so simple, I decided to do it in line. So let's talk about lambda and then come back to it. Lambda is used to create an anonymous function, anonymous in the sense that it has no name. So you start with the keyword lambda. You then give it a sequence of identifiers and then some expression. What lambda does is it builds a function that evaluates that expression on those parameters and returns the result of evaluating the expression. So instead of writing def, I have inline defined a function. So if we go back to it here, you can see that what I've done is lambda x one divided by Food.getCost of x. Notice food is the class name here. So I'm taking the function getCost from the class food, and I'm passing it the parameter x, which is going to be what? What's the type of x going to be? I can wait you out. What is the type of x have to be for this lambda expression to make sense? Well, go back to the class food. What's the type of the argument of getCost? What's the name of the argument to getCost? That's an easier question. We'll go back and we'll look at it. What's the type of the argument to getCost? AUDIENCE: Food. JOHN GUTTAG: Food. Thank you. So I do have-- speaking of food, we do have a tradition in this class that people who answer questions correctly get rewarded with food. Oh, Napoli would have caught that. So it has to be of type food because it's self in the class food. So if we go back to here, this x has to be of type food, right. And sure enough, when we use it, it will be. Let's now use it. I should point out that lambda can be really handy as it is here, and it's possible to write amazing, beautiful, complicated lambda expressions. And back in the good old days of 6001 people learned to do that. And then they learned that they shouldn't. My view on lambda expressions is if I can't fit it in a single line, I just go right def and write a function definition because it's easier to debug. But for one-liners, lambda is great.",
      "course": "6.0002",
      "lecture": "1 Introduction Optimization Problems MIT 60002 Intro to Computational Thinking and Data Science"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "179",
    "question": "What does it mean for relaxation to be safe?",
    "random_doc": {
      "doc_id": "6wUD_gp5WeE.en-qlPKC2UN_YU_1_srt",
      "content": "The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. JOHN GUTTAG: Today we're starting a new topic, which is, of course, related to previous topics. As usual, if you go to either the 60002 or the 600 web site, you'll find both today's PowerPoint and today's Python. You'll discover if you look at the Python file that there is quite a lot of code in there. And I'll be talking only about some of it. But it's probably all worth looking at. And a fair amount of reading associated with this week. Why are we looking at random walks? See a picture here of, think of them as molecules just bouncing around. This is actually a picture of what's called Brownian motion, though Robert Brown probably did not discover it. We're looking at random walks because, well, first of all, they're important in many domains. There are people who will argue, for example, that the movement of prices in the stock market is best modeled as a random walk. There was a very popular book called A Random Walk Down Wall Street that made this argument. And a lot of modern portfolio analysis is based upon that. Those of you who are not interested in making money, and I presume that's most of you, it's also very important in many physical processes. We use random walks, say, to model diffusion, heat diffusion, or the diffusion of molecules in suspension, et cetera. So they're very important in a lot of scientific, and indeed, social disciplines. They're not the only important thing, so why are we looking at those? Because I think it provides a really good illustration",
      "course": "6.0002",
      "lecture": "5 Random Walks"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "118",
    "question": "What is a dynamic array sequence in the context of data structures?",
    "random_doc": {
      "doc_id": "367ebcbfde99ec18e14e87b69f564035_MIT6_0002F16_lec12_15_pdf",
      "content": " \n \nIteration 3  \n6.0002  LECTURE 12 \n15 \n",
      "course": "6.0002",
      "lecture": "Lecture 12: Clustering"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "136",
    "question": "How can subtree properties in binary trees be efficiently maintained when nodes are added or removed?",
    "random_doc": {
      "doc_id": "9syvZr-9xwk.en-j3PyPqV-e1s_8_srt",
      "content": "Let's go on from there. So let's do a couple of examples. Here, again, is that same-- getting to be an old friend, that automaton M1. Remember, its language here is the set of strings that have the substring 11. That is that language A. Now, what do we know about A from the previous slide? Think with me. Don't just listen. A is a regular language now, because it's recognized by some automaton. So whenever you find an automaton for a language, a finite automaton for language, we know that that language is a regular language. So let's look at a couple of more examples. So if you take the language-- let's call this one B, which is the strings that have an even number of 1's in them. So like the string 1101, would that be in B? No, because it has an odd number of 1's. So the string 1111 has four 1's in it. That's an even number, so that string would be in B. The 0's don't matter for this language. So strings that have an even number of 1's, that's a regular language. And the way you would know that is you would have to make a finite automaton that recognizes that language. And I would encourage you to go and make that automaton. You can do it with two states. It's a very simple automaton. But if you haven't had practice with these, I encourage you to do that. And actually, there are lots of examples that I ask you to solve at the end of chapter 1 in the book, and you definitely should spend some time playing with it if you have not yet seen finite automata before. You need to get comfortable with these and be able to make them. So we're going to start making some of them, but we're going to be talking about it at a sort of a more abstract level in a minute. Basically, the reason why you can solve this problem, you can make a finite automaton which recognizes the language B, is because that finite automaton is going to keep track of the parity of the number of 1's it's seen before. This has two states, one of them remembering that it's seen an odd number of 1's so far, the other one remembering it's seen an even number of 1's before. And that's going to be typical for these automata, finite automata. There's going to be several different possibilities that you may have to keep track of as you're reading the input, and there's going to be a state associated with each one of those possibilities. So if you're designing an automaton, you have to think about-- as you're processing the input-- what things you have to keep track of. And you're going to make a state for each one of those possibilities. OK? So you need to get comfortable with that. Let's look at another example, the language C where the inputs have an equal number of 0's and 1's. That turns out to be not a regular language. So, in other words, what that means is there's no way to recognize that language with a finite automaton. You just can't do it. That's beyond the capabilities of finite automata. And that's a statement we will prove later. OK. And our goal over the next lecture or so is to understand the regular languages, which you can do in a very comprehensive way.",
      "course": "18.404J",
      "lecture": "1 Introduction Finite Automata Regular Expressions"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "175",
    "question": "What is the goal of the picture hanging problem as described?",
    "random_doc": {
      "doc_id": "02f2ed5e44be5415b2cba8175c924092_MIT6_0002F16_lec8_18_pdf",
      "content": " \n \n \n \n \nSample Size and Standard Deviation  \n6.0002  LECTURE 8 \n \n18\n",
      "course": "6.0002",
      "lecture": "Lecture 8: Sampling and Standard Error"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "244",
    "question": "What are the core concepts in graph-theoretic models?",
    "random_doc": {
      "doc_id": "iZPzBHGDsWI.en-j3PyPqV-e1s_5_srt",
      "content": "OK, here we go. So I'm just going to give a reduction. That's what the definition means. I'm going to give a way of converting formulas to pairs, a G and a k, where the formula's going to be satisfiability if and only if the graph has a k-clique. OK, so let's a little bit do it by example. And in order to do that-- so here's going to be a formula now. It's in 3CNF. That's what I need in order to be doing this reduction. I'm converting 3CNF formulas into clique problems. We to have a little bit understand what it means when we say-- we talk about the satisfiability of a formula like this, because it's going to be helpful in doing the reduction. Obviously, satisfiability means that the-- you can find an assignment to the variables. So you're going to set each of these variables-- A, B, and C, and so on-- to true or false, and you want to make the whole formula evaluate the true. But what does that actually mean? It means that, because of the structure of the formula, that making this formula true corresponds to making each clause true-- because the clauses are all anded together, so the only way for the formula to be true is to make each clause true. And to make a clause true, you have to make at least one of the literals true. So it's another way of thinking about satisfying this formula. Satisfying these [INAUDIBLE] satisfying assignment makes at least one true literal in every clause. It's really important to think about it that way, because that's what's going to be the basis for doing this reduction and all of the reductions. It's what makes 3SAT easy to think about, in terms of its satisfiability. If you had a general satisfiability problem and you had a satisfiability both formula, there's no obvious way of seeing what the satisfying assignment looks like, but here we understand what it looks like. It has that very special form, making one true literal in every clause-- at least one true literal in every clause. So now we're going to do the reduction. So I'm going to take from this formula-- I know, for some of you, you're going to be chafing. Why am I going slowly? But I want to make sure that we're all together and understanding what the rules of the game are and what we're trying to do. We're trying to convert this formula into a graph and a number. So right now my job is, to do this reduction, is to exhibit that graph. So I'm going to do that and two steps. First, I'm going to tell you what the nodes of the graph are. Then I'll tell you what the edges of that graph [AUDIO OUT] Finally, I'll tell you what the number k is. That's the way this polynomial time reduction is going to work. And we have to also observe at the very end that the reduction that I'm given-- giving you, this procedure for building this graph can be done in polynomial time, but that, I think-- you'll see, once I'm done, that that's pretty obvious. OK, so first, as I promised, the nodes-- so the nodes of this graph are going to correspond to the literals of the formula. Every literal is going to become a node in the graph, and it's going to be labeled with the name of that literal. Every node is going to be labeled an a, a b, or c bar, and so on. So here it goes.",
      "course": "18.404J",
      "lecture": "15 NP-Completeness"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "104",
    "question": "What is a rejecting computation history?",
    "random_doc": {
      "doc_id": "7J1HD9rqEB4.en-j3PyPqV-e1s_3_srt",
      "content": "using the arithmetization method. So first of all, arithmetization is a simulation of and and or with plus and times, such that if I think about true as a 1 and false as a 0, this is going to give me a faithful simulation. It's going to do the right thing. It's going to compute exactly the same values that we expect. So like a and b, well, times works just like and. It does for 1 and 0 as true and false, times exactly works like and. And negation is 1 minus. Or is going to be the sum minus the product. And then, these just give you the right values, a or b. If you just calculate it out by plugging in 1s or 0s, you get the right answer just by using this arithmetic. So now, what we're going to do, instead of using the Boolean labeling, we'll just use the arithmetical labeling. But it's going to compute exactly the same thing because the arithmetic simulates the Boolean. So we always go through the start node. So there's no question about labeling the very start node with a 1. But now, I'm going to give expressions just like the Boolean expressions, but now they're going to use plus and times instead of ands and ors. So let's just see. Remember what we did from before. We had a and xi for this edge. I'm going to replace that. What is and? We just look up here in our table, in our translation table. And becomes times. So we're going to replace that with a times xi. And it's going to work exactly the same. But the difference is that this makes sense even when we have non-Boolean values. Times and plus are defined for non-Boolean values, whereas ands and or are not. So what goes down on this edge? Well, this was a and the complement of xi, as you remember. So that's going to become a times 1 minus xi. And then similarly, we had or over here. And here's a little bit of a trick, but that's going to be important for the analysis that we're going to do. Instead of using the recipe for or in terms of plus and times, we're going to have something a little simpler. It's just going to be the sum. And the reason why that works-- good to understand-- is that because of the acyclic nature of the branching programs, at most, one of these edges can have a path through it. So this is a kind of very special or, sometimes called the disjoint or. You're not allowed to have more than one of the values be 1, because that never happens when you have an acyclical graph. You can never have the path coming down this way, and then, again, coming down that way. Then it would be entering that node twice. Have to be a cycle. So it's going to be good enough for us, and necessary for us to represent this or as a sum. OK. So I think that's all I wanted to say on this slide. So somebody is asking, is it possible for some of these values to be negative? Yes. As it stands right now, some of these values can be negative. I haven't put any restriction on what the values are going to be. So the input could be a negative number. And then, you're going to just get negative stuff happening. In fact, there's subtractions going on here. So even with positive numbers-- I think we did an example last time. I think I'm going to do that example again of exclusive or, where you get negative numbers coming up. That doesn't matter. But actually, what we're going to end up doing is doing these calculations modulo some prime number q. OK. I'm going to pick some prime like 17, and do all the calculations mod 17. And the reason for doing it that way is really because we're going to be picking random assignments to the variables as our input. And it makes the most sense to do that when you have a finite set of possibilities to pick them up. So we're not going to pick like a random integer. There's infinitely many possibilities. And yeah, you could set up a distribution there, but that's very complicated. That actually might work. I'm not sure. I haven't actually gone through that analysis. But the typical way people do this is by looking at what's called a finite field. So I'll talk about that in a second. Why is there at most one 1 among a1, a2, and a3? The 1s-- I'll say once again-- but the 1s correspond to the path. So this is a 1 if the path went this way. Just think about it. The path cannot go through a1 and can, at the same time, go through a2, because that means the path went through this node. Then, how is it going to get over to a2? It's going to go through that node twice. In an acyclic graph, you cannot have a path going through it's the same node more than once. So you're going to have to think about that.",
      "course": "18.404J",
      "lecture": "24 Probabilistic Computation cont"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "12",
    "question": "Why is proving a problem to be NP-complete considered strong evidence that it lacks a polynomial time solution?",
    "random_doc": {
      "doc_id": "28461a74f81101874a13d9679a40584d_MIT6_006S20_lec16_3_pdf",
      "content": "3 \nLecture 16: Dyn. Prog. Subproblems \n6. Time \n• # subproblems: (|A| + 1)  · (|B| + 1)  \n• work per subproblem: O(1) \n• O(|A| · |B|) running time \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \ndef lcs(A, B): \na, b = len(A), len(B) \nx = [[0] * (b + 1) for _ in range(a + 1)] \nfor i in reversed(range(a)): \nfor j in reversed(range(b)): \nif A[i] == B[j]: \nx[i][j] = x[i + 1][j + 1] + 1 \nelse: \nx[i][j] = max(x[i + 1][j], x[i][j \nreturn x[0][0] \n+ 1]) \n",
      "course": "6.006",
      "lecture": "Lecture 16 - Dynamic Programming, Part 2 - LCS, LIS, Coins"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "335",
    "question": "What are some of the reasons random walks are considered significant in various fields?",
    "random_doc": {
      "doc_id": "0a353b26f1c6bd161b28b3f249aa05d1_MIT6_0002F16_lec1_20_pdf",
      "content": "\u0002\u0007\u0010\u0016?\u001f\u0006%\u0018\u0018,\u0006\u001d\u0016\u0006\u001d\u0006\u000f\u0011\u0018\u0017\u0011\u001d\u001e\u0006\u0016\"\u001d\u0016\u0006\u0015\u0010\u0006\u0014\u001d\u0013\u0006)\u001f\u0010\u0006\u0016\u0018\u0006\u001c\u0010\u0014\u0012\u001c\u0010\u0006\u0015\"\u001d\u0016\u0006\n\u0016\u0018\u0006\u0018\u0011\u001c\u0010\u0011\n&\u000b\u001a\u0014\u0003\b\n\u0002\u0003\u0004\u0004\u0004\u0005\u0006\u0007\b\t\n\u000b\f\b\u0006\r\n\u0005\u0004\n\r\b\b\u000e\n\u000f\u0007\t\u0004\n\u0010\u0004\u0004\u0011\n\u0012\u0007\u0013\u0013\u0014\n\u0010\u0003\u0011\u0015\u0004\u0011\n\u0016\u0011\u0007\u0004\u0005\n\u0017\b\u0018\u0004\n\u0014\u0012\u0012\u0019\u0004\n\u000e\b\t\u0003\u0006\n.\u001d%)\u0010\n:;\n;\u0004\n&\u0004\n'\u0004\n;\u0004\n9;\n;\u0004\n\r\u0004\n\u0014\u001d%\u0018\u0011\u0012\u0010\u001f\n\r\u0005&\n\r'2\n\u0005':\n&'2\n&\u0002'\n\r'\u0004\n;'\n\r;'\n",
      "course": "6.0002",
      "lecture": "Lecture 1: Introduction and Optimization Problems"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "135",
    "question": "What is the runtime complexity of the merge function in merge sort?",
    "random_doc": {
      "doc_id": "a22fce6f6824c53dbb79a0da786966a6_MIT18_404f20_lec4_1_pdf",
      "content": " \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n18.404/6.840 Lecture 4 \nLast time: \n- Finite automata → regular expressions \n- Proving languages aren’t regular \n- Context free grammars \nToday: (Sipser §2.2) \n- Context free grammars (CFGs) – definition \n- Context free languages (CFLs) \n- Pushdown automata (PDA) \n- Converting CFGs to PDAs \n1 \n",
      "course": "18.404J",
      "lecture": "Lecture 4 - Pushdown Automata, CFG ↔ PDA"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "56",
    "question": "Why is it impossible to determine whether a machine is looping or just taking a long time?",
    "random_doc": {
      "doc_id": "Nu8YGneFCWE.en-j3PyPqV-e1s_4_srt",
      "content": "So what do the leaves actually represent? Those represent outputs. I'm going to output something here. Yep? AUDIENCE: [INAUDIBLE] JASON KU: The number of-- OK. So what is the output to my search algorithm? Maybe it's the-- an index of an item that contains this key. Or maybe I return the item is the output-- the item of the thing I'm storing. And I'm storing n things, so I need at least n outputs, because I need to be able to return any of the items that I'm storing based on a different search parameter, if it's going to be correct. I actually need one more output. Why do I need one more output? If it's not in there-- so any correct comparison searching algorithm-- I'm doing some comparisons to find this thing-- needs to have at least n plus 1 leaves. Otherwise, it can't be correct, because I could look up the one that I'm not returning in that set and it would never be able to return that value. Does that make sense? Yeah? AUDIENCE: [INAUDIBLE] JASON KU: What's n? For a data structure, n is the number of things stored in that data structure at that time-- so the number of items in the data structure. That's what it means in all of these tables. Any other questions? OK, so now we get to the fun part. How many comparisons does this algorithm have to do? Yeah, up there-- AUDIENCE: [INAUDIBLE] JASON KU: What's up? All right, your colleague is jumping ahead for a second, but really, I have to do as many comparisons in the worst case as the longest root-to-leaf path in this tree-- because as I'm executing this algorithm, I'll go down this thing, always branching down, and at some point, I'll get to a leaf. And in the worst case, if I happen to need to return this particular output, then I'll have to walk down the longest thing, just the longest path. So then the longest path is the same as the height of the tree, so the question then becomes, what is the minimum height of any binary tree that has at least n plus 1 leaves? Does everyone understand why we're asking that question? Yeah? AUDIENCE: Could you over again why it needs n plus 1 leaves? JASON KU: Why it needs n plus 1 leaves-- if it's a correct algorithm, it needs to return-- it needs to be able to return any of the n items that I'm storing or say that the key that I'm looking for is not there-- great question. OK, so what is the minimum height of any binary tree that has n plus 1-- at least n plus 1 leaves? You can actually state a recurrence for that and solve that. You're going to do that in your recitation. But it's log n. The best you can do is if this is a balanced binary tree. So the min height is going to be at least log n height. Or the min height is logarithmic, so it's actually theta right here. But if I just said height here, I would be lower bounding the height. I could have a linear height, if I just changed comparisons down one by one, if I was doing a linear search, for example. All right, so this is saying that, if I'm just restricting to comparisons, I have to spend at least logarithmic time to be able to find whether this key is in my set. But I don't want logarithmic time. I want faster. So how can I do that? AUDIENCE: [INAUDIBLE] JASON KU: I have one operation in my model of computation I presented a couple of weeks ago that allows me to do faster, which allows me to do something stronger than comparisons. Comparisons have a constant branching factor. In particular, I can-- if I do this operation-- this constant time operation-- I can branch to two different locations. It's like an if kind of situation-- if, or else. And in fact, if I had constant branching factor for any constant here-- if I had three or four, if it was bounded by a constant, the height of this tree would still be bounded by a log base the constant of that number of leaves. So I need, in some sense, to be able to branch a non-constant amount. So how can I branch a non-constant amount? This is a little tricky. We had this really neat operation in the random access machine that we could randomly go to any place in memory in constant time based on a number. That was a super powerful thing, because within a single constant time operation, I could go to any space in memory. That's potentially much larger than linear branching factor, depending on the size of my model and the size of my machine. So that's a very powerful operation. Can we use that to find quicker? Anyone have any ideas? Sure. AUDIENCE: [INAUDIBLE] JASON KU: We're going to get to hashing in a second, but this is a simpler concept than hashing-- something you probably are familiar with already. We've kind of been using it implicitly in some of our sequence data structure things. What we're going to do is, if I have an item that has key 10, I'm going to keep an array and store that item 10 spaces away from the front of the array, right at index 9, or the 10th index. Does that make sense? If I store that item at that location in memory, I can use this random access to that location and see if there's something there. If there's something there, I return that item. Does that make sense? This is what I call a direct access array.",
      "course": "6.006",
      "lecture": "4 Hashing"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "191",
    "question": "What is the running time of the direct access array sort if the range of keys \\( u \\) is \\( \\Theta(n) \\)?",
    "random_doc": {
      "doc_id": "50cb369d1be3c7fbe0886e318aea13c2_MIT18_404f20_lec22_9_pdf",
      "content": " \n \n \n  \n \n \n \n  \n \n \n  \n \n \n  \n \n \n  \n \n \n \n \n  \n \n  \n  \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n  \n \n \n \n \n \n \n \n \n  \n   \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n  \n  \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nNO.\nOracles and P versus NP \nTheorem: There is an oracle ! where P \" = NP \" \nProof: Let ! = $%&' \nNP()*+ ⊆ NPSPACE = PSPACE ⊆ P()*+ \nRelevance to the P versus NP question \nRecall: We showed -%./0↑ ∉ PSPACE. \nCould we show 3!$ ∉ P using a similar method? \nReason: Suppose YES. \nThe Hierarchy Theorems are proved by a diagonalization. \nIn this diagonalization, the TM 4 simulates some TM 5. \nIf both TMs were oracle TMs 4\" and 5\" with the same oracle !, \nthe simulation and the diagonalization would still work. \nTherefore, if we could prove P ≠ NP by a diagonalization, \nwe would also prove that P \" ≠ NP \" for every oracle !. \nBut that is false! \n9 \nCheck-in 22.3 \nWhich of these are known to be true? \nCheck all that apply. \nP7\"( \nP7\"( \n(a) \n= \n(b) NP7\"( = coNP7\"( \n(c) MIN-FORMULA ∈ P()*+ \nNP()*+ = coNP()*+ \n(d) \nCheck-in 22.3 \n",
      "course": "18.404J",
      "lecture": "Lecture 22 - Provably Intractable Problems, Oracles"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "210",
    "question": "What is the definition of a negative-weight cycle in a graph?",
    "random_doc": {
      "doc_id": "IycOPFmEQk8.en-j3PyPqV-e1s_5_srt",
      "content": "And there is a natural ambiguity that comes up in a programming language. If you have if some condition then statement one, else statement two, I presume you understand what the semantics of that is, what that means. And the tricky thing is that if you have-- those statements can themselves be if statements. And so if you have the situation where you have if then and if then else is what follows that, the question is, where does the else attach? Is it to the second if or to the first if? So that's kind of a big hint on this problem, but that's OK. You need to take that and figure out how to get an actual member of the language which is ambiguously generated, and then show that it has-- show that it is by showing two parse trees or two leftmost derivations. If you read the book, you'll see that's an alternative way of representing a parse tree. So and then what you're supposed to do is give a grammar for the same language which is unambiguous. You don't have to prove that it's unambiguous, because that's a bit of a chore. But as long as you understand what's going on, you should be able to come up with an unambiguous grammar which resolves that ambiguity. And I don't have in mind changing the language by introducing new programming language constructs like a \"begin end.\" That's not in the spirit of this problem, because that's a different-- it's grammar for a different language. So you need to be generating the same language without any other extraneous things going on that are going to resolve the ambiguity. The ambiguity needs to be resolved within the structure of the grammar itself. So keep that in mind. For problem number three about the queue automata, you know, that came up actually as a suggestion last lecture, I believe, or two lectures back. What happens if you take a pushdown automaton, but instead of a pushdown-- instead of a stack, you add a queue. What happens then? Well actually, it turns out that the model you get is very powerful. And it turns out to be equivalent in power to a Turing machine. So you'll see arguments of that kind today, how you show that other models are equivalent-- no, not today. So I apologize. This is going to be something that you'll-- I'm confusing myself here. Problem number three actually needs Thursday's lecture as well to really at least see examples of how you do that kind of thing. Yeah, so I'll try to send out a note clarifying this. By the end of Thursday, you'll be able to do everything, except for problem six. And for problem six, you'll need Tuesday's lecture, a week from today's lecture, to do. So problem number four, that one you'll be able to do at the end of today. That's also going to-- the problem is I'm working on preparing Thursday's lecture too. So I'm getting a little-- I'm confusing myself. Problem number four, you'll be able to do after Thursday's lecture. Maybe we should talk about that next lecture. Problem number five, you can do today, but maybe I'm not going to say anything about that. And problem number six, I won't say anything about either. OK, so why don't we just jump in then and look at today's material. What about seven? Oh, seven is an optional problem. Oh, I should have mentioned that. Seven is always going to be an option. I indicate that with a star I should have made that clear on the actual description here, but seven is optional. It's just like we had for problem set one. OK, let's move let's move on, then, to what we're going to talk about today. And just a little bit of review-- so we talked about the equivalence of context-free grammars and pushdown automata, as you remember. Oops, let me get myself out of the picture here. As we mentioned last time, we actually proved one direction, but the other direction of that, you just have to know it's true, but you don't have to know the proof. The proof is a little bit lengthy, I would say. It's a nice proof, but it's pretty long. And there are two important corollaries to that. If you know what a corollary is, it's just a simple consequence which doesn't need much of a proof, sort of a very straightforward consequence. First of all, I think we pointed out last time, one conclusion, one corollary you get is that every regular language is a context-free language, because a finite automaton is a pushdown automaton that just happens not to use its stack. So immediately, you get that every language is context free. And second of all, you also immediately get that whenever you have a context-free language and a regular language and you take their intersection, you get back a context-free language. So context free intersect regular is context free. That's actually mentioned in your homework as well as one of the 0.x problems which I give to try to get you-- you don't have to turn those in, but I suggest you look at them. I don't know how many of you are looking at them. But this is a useful fact. And some of those other facts in 0.x problems are useful. So I encourage you to look at them. But anyway, intersection of context free and regular is context free. You might ask, what about intersection of context free and context free? Do we have closure under intersection? The answer is, no, we do not have close to closure under intersection.",
      "course": "18.404J",
      "lecture": "5 CF Pumping Lemma Turing Machines"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "60",
    "question": "What is a Generalized Nondeterministic Finite Automaton (GNFA) and how does it differ from a standard NFA in terms of transition labels?",
    "random_doc": {
      "doc_id": "4dFPVJrNLDs.en-j3PyPqV-e1s_11_srt",
      "content": "So here's the polynomial time algorithm. You construct the graph, you accept if there's a path from the start to the accept, and you reject if there is not. And so that tells us that not only is L contained within NL, but NL itself is also contained within P. So here's a kind of a nice hierarchy of languages. Not only do we not know whether L equals NL, we don't know whether L equals P. It's possible that anything you can do in polynomial time, you can do deterministically in log space, shocking as that might be, because this is a pretty weak class. But we don't know how to prove that there's anything different, anything in P that's not in here. Last check in. So we showed that PATH is in NL. What's the best thing we can do about the deterministic space complexity of PATH? So deterministic. So this is nondeterministic log space. What can we say deterministically about PATH? Hint-- this should not be hard if you think back to what we've shown very recently. Get your check in points. Closing up. Closing shop here. All set, 1, 2, 3. Yeah, so the correct answer is log squared space, because this is just Savitch's theorem. We can do it in log space nondeterministically, so you can do it in log squared space deterministically. So this is what we did today. And as I mentioned, I will do this again on Tuesday's lecture, just to recap that. All right, so I'll stick around a little bit for questions. And someone's asking me about the nomenclature. Why is it L and not L space? Because people don't usually talk about L time. So L is sort of-- everybody knows the only reasonable option is space, so people just say L and NL. I mean, some of these names have a little bit evolved over time. And even now, some people talk about-- I call \"time classes,\" some people call \"detime classes.\" You can make different choices there. Let's see. Good.",
      "course": "18.404J",
      "lecture": "19 Games Generalized Geography"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "184",
    "question": "How can the parent node index be computed from a given left or right child node index in a binary heap stored as an array?",
    "random_doc": {
      "doc_id": "f9cVS_URPc0.en-j3PyPqV-e1s_9_srt",
      "content": "All right. So we construct this graph G prime. We can do that in linear time with respect to these things. I just go through all the edges, I make these edges, and I make these vertices. It doesn't take anything-- I just do it naively. Right I can do that in time V times V plus E asymptotically. OK. Now I run DAG relaxation, our nice algorithm we had last time, from-- in there was a0. I'm going to say it's S0, our source. Our source vertex. Single source shortest paths. So that I compute delta of S0 to Vk for all k and-- what is it? 0 to V. That's what single source shortest paths does. It computes for me this distance from my source-- at some source to every other vertex in the graph. And so in particular I get these. Well, that is all of them. Then for each vertex V, set-- the thing I'm going to return, d-value, S to V, equal to the shortest-path distance I got from DAG relaxation to a particular vertex. V V minus 1. Why am I doing this? I'm setting it to be the shortest-path distance to the guy in the second-to-last row here or column in my modified graph. The hope is that this distance in my DAG corresponds to this distance in my original graph. The distance to V using at most V minus 1 edges. So that's the claim-- that's a claim we're going to prove in just a second. I'm going to write it down just so that we have-- just to continue our train of thought. Claim, delta S0 Vk equals delta k, the k edge distance, from S to V. That's what we want to claim. That would then-- what would that mean, then? That would mean that I'm correctly setting the shortest-path distance here for all vertices whose distances finite. Great. I mean, I set values to things where they're not finite, where they're minus infinity also, but in particular I set the ones correctly if they're finite. OK. So the last thing we need to do is deal with these minus infinity vertices. But we know how to do that. We just look at the witnesses. Because we've computed this value for k equals V equals V minus 1, and if that claim over there is true, then those shortest-path distances are the same as these k edge shortest-path distances. And we can just, for every vertex, we compare these things. If this is satisfied, we got a witness. OK. So for each witness U and V where delta S0 U V is less than, strictly, S0 U V minus 1-- that's the definition of a witness here, close the parentheses. Then for each vertex V reachable from U set-- sorry, d, is what we're returning, d of S, V equal to minus infinity. That's the end of the algorithm. Basically I'm looking for all the witnesses. For each witness, I find all of the vertices reachable from it and set it to minus infinity just as we argued before. OK. So, it remains to prove this claim. How do we prove this claim? Well, we can induct on k. Is this true for k equals 0? Yeah. We kind of already argued it over here when we are talking about our initialization step or what DAG relaxation does. It'll set this to be the shortest path from this guy to all these vertices. These aren't reachable from here, and so these are infinite.",
      "course": "6.006",
      "lecture": "12 Bellman-Ford"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "200",
    "question": "What is the difference between an interface and a data structure?",
    "random_doc": {
      "doc_id": "0a353b26f1c6bd161b28b3f249aa05d1_MIT6_0002F16_lec1_10_pdf",
      "content": "3\u0003\f\u000e\u0015\f\t-\u000b\u0011\u0005\u0006\u0012\u0013\u0014\u000f\u0015\n\u0002\u0003\u0004\u0004\u0004\u0005\u0006\u0007\b\t\n\u000b\f\b\u0006\r\n\r\u0004\nImages © sources unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use.\n",
      "course": "6.0002",
      "lecture": "Lecture 1: Introduction and Optimization Problems"
    },
    "relevance": "notrelevant"
  },
  {
    "query_id": "249",
    "question": "Why is fair roulette considered a better bet than European or Las Vegas roulette?",
    "random_doc": {
      "doc_id": "oS9aPzUNG-s.en-j3PyPqV-e1s_5_srt",
      "content": "interface a little bit more. So our set is a container. It contains all of the students in this classroom, in some virtual sense at least. And so to build up our set, of course, we need an operation that takes some iterable object A and builds a set out of it. So in other words, I have all the students in this classroom represented maybe in some other fashion. And I have to insert them all into my set. I can also ask my set for how much stuff is in it. Personally, I would call that size. But length is cool, too. And then of course, there are a lot of different ways that we can interact with our set. So for instance, we could say, is this student taking 6.006? So in set language, one way to understand that is to say that the key-- each person in this classroom is associated with a key. Does that key k exist in my set? In which case, I'll call this find function, which will give me back the item with key k or maybe null or something if it doesn't exist. Maybe I can delete an object from my set or insert it. Notice that these are dynamic operations, meaning that they actually edit what's inside of my set. And then finally, there are all kinds of different operations that I might want to do to interact with my set beyond is this thing inside of it. So for instance, so for the student ID example, probably finding the minimum ID number in a class isn't a terribly exciting exercise. But maybe I'm trying to find the student who's been at MIT the longest. And so that would be a reasonable heuristic. I actually have no idea whether MIT student IDs are assigned linearly or not. But in any event, I could find the smallest key, the largest key, and so on in my set. And these are all reasonable operations to query, where my object is just",
      "course": "6.006",
      "lecture": "3 Sets and Sorting"
    },
    "relevance": "notrelevant"
  }
]
