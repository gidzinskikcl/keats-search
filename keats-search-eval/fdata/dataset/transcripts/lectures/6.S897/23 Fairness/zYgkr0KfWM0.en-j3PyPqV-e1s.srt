1
00:00:00,000 --> 00:00:15,920

2
00:00:15,920 --> 00:00:19,520
PETER SZOLOVITS: OK,
so a little over a year

3
00:00:19,520 --> 00:00:24,620
ago, I got a call
from this committee.

4
00:00:24,620 --> 00:00:28,550
NASEM is the National Academy
of Science, Engineering,

5
00:00:28,550 --> 00:00:30,200
and Medicine.

6
00:00:30,200 --> 00:00:36,830
So this is an august body of old
people with lots of gray hair

7
00:00:36,830 --> 00:00:40,130
who have done something
important enough to get elected

8
00:00:40,130 --> 00:00:41,870
to these academies.

9
00:00:41,870 --> 00:00:45,590
And their research arm is called
the National Research Council

10
00:00:45,590 --> 00:00:48,050
and has a bunch of
different committees.

11
00:00:48,050 --> 00:00:50,750
One of them is this Committee
on Science, Technology,

12
00:00:50,750 --> 00:00:52,033
and the Law.

13
00:00:52,033 --> 00:00:53,450
It's a very
interesting committee.

14
00:00:53,450 --> 00:00:55,880
It's chaired by
David Baltimore, who

15
00:00:55,880 --> 00:01:00,260
used to be an MIT professor
until he went and became

16
00:01:00,260 --> 00:01:02,750
president of Caltech.

17
00:01:02,750 --> 00:01:05,990
And he also happens to have
a Nobel Prize in his pocket

18
00:01:05,990 --> 00:01:09,410
and he's a pretty famous guy.

19
00:01:09,410 --> 00:01:13,220
And Judge David
Tatel is a member

20
00:01:13,220 --> 00:01:16,190
of the US Court of Appeals
for the District of Columbia

21
00:01:16,190 --> 00:01:21,260
circuit, so this is probably the
most important circuit court.

22
00:01:21,260 --> 00:01:24,110
It's one level below
the Supreme Court.

23
00:01:24,110 --> 00:01:26,360
And he happens to
sit in the seat

24
00:01:26,360 --> 00:01:28,910
that Ruth Bader
Ginsburg occupied

25
00:01:28,910 --> 00:01:32,120
before she was elevated
to the Supreme Court

26
00:01:32,120 --> 00:01:36,240
from that Court of Appeals,
so this is a pretty big deal.

27
00:01:36,240 --> 00:01:38,970
So these are heavy hitters.

28
00:01:38,970 --> 00:01:45,510
And they convened a meeting to
talk about the set of topics

29
00:01:45,510 --> 00:01:47,700
that I've listed here.

30
00:01:47,700 --> 00:01:51,240
So blockchain and distributed
trust, artificial intelligence

31
00:01:51,240 --> 00:01:54,660
and decision making, which is
obviously the part that I got

32
00:01:54,660 --> 00:01:58,140
invited to talk about,
privacy and informed consent

33
00:01:58,140 --> 00:02:01,560
in an era of big data,
science curricula for law

34
00:02:01,560 --> 00:02:06,340
schools, emerging issues, and
science, technology, and law.

35
00:02:06,340 --> 00:02:09,900
The issue of using litigation
to target scientists who have

36
00:02:09,900 --> 00:02:12,380
opinions that you don't like.

37
00:02:12,380 --> 00:02:15,900
And the more general
issue of how do you

38
00:02:15,900 --> 00:02:20,730
communicate advances in life
sciences to a skeptical public.

39
00:02:20,730 --> 00:02:24,870
So this is dealing with the
sort of anti-science tenor

40
00:02:24,870 --> 00:02:27,030
of the times.

41
00:02:27,030 --> 00:02:31,710
So the group of us that talked
about AI and decision making,

42
00:02:31,710 --> 00:02:34,410
I was a little bit
surprised by the focus

43
00:02:34,410 --> 00:02:38,520
because Hank really is a law
school professor at Stanford

44
00:02:38,520 --> 00:02:43,290
who's done a lot of work
on fairness and prejudice

45
00:02:43,290 --> 00:02:47,380
in health care.

46
00:02:47,380 --> 00:02:51,540
Cherise Burdee is at something
called the Pretrial Justice

47
00:02:51,540 --> 00:02:54,600
Institute, and her
issue is a legal one

48
00:02:54,600 --> 00:02:58,770
which is that there are
now a lot of companies that

49
00:02:58,770 --> 00:03:03,330
have software that predict,
if you get bail while you're

50
00:03:03,330 --> 00:03:07,450
awaiting trial, are you
likely to skip bail or not?

51
00:03:07,450 --> 00:03:10,350
And so this is influential
in the decision

52
00:03:10,350 --> 00:03:13,650
that judges make about
how much bail to impose

53
00:03:13,650 --> 00:03:17,670
and whether to let
you out on bail at all

54
00:03:17,670 --> 00:03:21,210
or to keep you in prison,
awaiting your trial.

55
00:03:21,210 --> 00:03:25,710
Matt Lundgren is a radiology
professor at Stanford

56
00:03:25,710 --> 00:03:28,020
and has done some of
the really cool work

57
00:03:28,020 --> 00:03:31,530
on building convolutional
neural network

58
00:03:31,530 --> 00:03:36,140
models to detect pulmonary
emboli and various other things

59
00:03:36,140 --> 00:03:40,560
in imaging data.

60
00:03:40,560 --> 00:03:47,130
You know the next guy, and
Suresh Venkatasubramanian

61
00:03:47,130 --> 00:03:48,570
is a professor.

62
00:03:48,570 --> 00:03:52,830
He was originally a theorist
at the University of Utah

63
00:03:52,830 --> 00:03:56,520
but has also gotten into
thinking a lot about privacy

64
00:03:56,520 --> 00:03:58,290
and fairness.

65
00:03:58,290 --> 00:04:02,820
And so that that was our panel,
and we each gave a brief talk

66
00:04:02,820 --> 00:04:06,160
and then had a very
interesting discussion.

67
00:04:06,160 --> 00:04:09,660
One of the things that I was
very surprised by is somebody

68
00:04:09,660 --> 00:04:15,390
raised the question of shouldn't
Tatel as a judge on the Circuit

69
00:04:15,390 --> 00:04:18,420
Court of Appeals
hire people like you

70
00:04:18,420 --> 00:04:22,830
guys to be clerks in his court?

71
00:04:22,830 --> 00:04:25,080
So people like you
guys who also happen

72
00:04:25,080 --> 00:04:29,250
to go to law school, of
which there are a number now

73
00:04:29,250 --> 00:04:34,230
of people who are trained
in computational methods

74
00:04:34,230 --> 00:04:39,300
and machine learning but also
have the legal background.

75
00:04:39,300 --> 00:04:42,030
And he said something
very interesting to me.

76
00:04:42,030 --> 00:04:44,250
He said, no, he
wouldn't want people

77
00:04:44,250 --> 00:04:48,585
like that, which
kind of shocked me.

78
00:04:48,585 --> 00:04:52,410
And so we quizzed him
a little bit on why,

79
00:04:52,410 --> 00:04:58,830
and he said, well, because he
views the role of the judge

80
00:04:58,830 --> 00:05:02,490
not to be an expert
but to be a judge.

81
00:05:02,490 --> 00:05:07,590
To be a balancer of arguments
on both sides of an issue.

82
00:05:07,590 --> 00:05:10,740
And he was afraid that
if he had a clerk who

83
00:05:10,740 --> 00:05:13,530
had a strong
technical background,

84
00:05:13,530 --> 00:05:16,560
that person would have strong
technical opinions which

85
00:05:16,560 --> 00:05:20,110
would bias his decision
one way or another.

86
00:05:20,110 --> 00:05:21,600
So this reminded me--

87
00:05:21,600 --> 00:05:23,820
my wife was a lawyer,
and I remember,

88
00:05:23,820 --> 00:05:27,870
when she was in law school, she
would tell me about the classes

89
00:05:27,870 --> 00:05:29,430
that she was taking.

90
00:05:29,430 --> 00:05:35,250
And it became obvious
that studying law

91
00:05:35,250 --> 00:05:41,220
was learning how to win, not
learning how to find the truth.

92
00:05:41,220 --> 00:05:43,350
And there's this
philosophical notion

93
00:05:43,350 --> 00:05:47,010
in the law that says that
the truth will come out

94
00:05:47,010 --> 00:05:51,300
from spirited argument on
two sides of a question,

95
00:05:51,300 --> 00:05:55,500
but your duty as a lawyer is
to argue as hard as you can

96
00:05:55,500 --> 00:05:58,500
for your side of the argument.

97
00:05:58,500 --> 00:06:02,460
And in fact, in law school,
they teach them, like in debate,

98
00:06:02,460 --> 00:06:06,540
that you should be able to
take either side of any case

99
00:06:06,540 --> 00:06:09,750
and be able to make a
cogent argument for it.

100
00:06:09,750 --> 00:06:13,650
And so Tatel sort of
reinforced that notion

101
00:06:13,650 --> 00:06:17,520
in what he said, which I
thought was interesting.

102
00:06:17,520 --> 00:06:21,210
Well, just to talk a little
bit about the justice area

103
00:06:21,210 --> 00:06:24,180
because this is the one
that has gotten the most

104
00:06:24,180 --> 00:06:29,970
public attention, governments
use decision automation

105
00:06:29,970 --> 00:06:34,260
for determining eligibility
for various kinds of services,

106
00:06:34,260 --> 00:06:37,230
evaluating where to deploy
health inspectors and law

107
00:06:37,230 --> 00:06:40,380
enforcement personnel,
defining boundaries

108
00:06:40,380 --> 00:06:42,060
along voting districts.

109
00:06:42,060 --> 00:06:46,820
So all of the gerrymandering
discussion that you hear about

110
00:06:46,820 --> 00:06:50,070
is all about using
computers and actually

111
00:06:50,070 --> 00:06:53,910
machine learning techniques
to try to figure out how to--

112
00:06:53,910 --> 00:06:57,930
your objective function is to
get Republicans or Democrats

113
00:06:57,930 --> 00:07:02,010
elected, depending on who's in
charge of the redistricting.

114
00:07:02,010 --> 00:07:06,690
And then you tailor these
gerrymandered districts

115
00:07:06,690 --> 00:07:10,710
in order to maximize the
probability that you're

116
00:07:10,710 --> 00:07:16,140
going to have the majority in
whatever congressional races

117
00:07:16,140 --> 00:07:21,300
or state legislative races.

118
00:07:21,300 --> 00:07:27,110
So in the law, people are
in favor of these ideas

119
00:07:27,110 --> 00:07:30,170
to the extent that they
inject clarity and precision

120
00:07:30,170 --> 00:07:33,170
into bail, parole, and
sentencing decisions.

121
00:07:33,170 --> 00:07:35,300
Algorithmic technologies
may minimize

122
00:07:35,300 --> 00:07:38,150
harms that are the
products of human judgment.

123
00:07:38,150 --> 00:07:41,840
So we know that people
are in fact prejudiced,

124
00:07:41,840 --> 00:07:46,520
and so there are prejudices
by judges and by juries

125
00:07:46,520 --> 00:07:52,020
that play into the decisions
made in the legal system.

126
00:07:52,020 --> 00:07:56,450
So by formalizing
it, you might win.

127
00:07:56,450 --> 00:07:59,120
However, conversely,
the use of technology

128
00:07:59,120 --> 00:08:02,840
to determine whose liberty
is deprived and on what terms

129
00:08:02,840 --> 00:08:06,140
raises significant
concerns about transparency

130
00:08:06,140 --> 00:08:07,880
and interpretability.

131
00:08:07,880 --> 00:08:10,880
So next week, we're going to
talk some about transparency

132
00:08:10,880 --> 00:08:14,000
and interpretability,
but today's is really

133
00:08:14,000 --> 00:08:16,890
about fairness.

134
00:08:16,890 --> 00:08:21,165
So here is an article from
October of last year--

135
00:08:21,165 --> 00:08:24,440
no, September of last year,
saying that as of October

136
00:08:24,440 --> 00:08:28,160
of this year, if you get
arrested in California,

137
00:08:28,160 --> 00:08:31,040
the decision of whether
you get bail or not

138
00:08:31,040 --> 00:08:33,289
is going to be made by
a computer algorithm,

139
00:08:33,289 --> 00:08:36,059
not by a human being, OK?

140
00:08:36,059 --> 00:08:41,169
So it's not 100%.

141
00:08:41,169 --> 00:08:46,970
There is some discretion on the
part of this county official

142
00:08:46,970 --> 00:08:50,090
who will make a recommendation,
and the judge ultimately

143
00:08:50,090 --> 00:08:53,990
decides, but I suspect
that until there

144
00:08:53,990 --> 00:08:58,520
are some egregious
outcomes from doing this,

145
00:08:58,520 --> 00:09:02,420
it will probably be
quite commonly used.

146
00:09:02,420 --> 00:09:05,970

147
00:09:05,970 --> 00:09:10,230
Now, the critique of
these bail algorithms

148
00:09:10,230 --> 00:09:14,500
is based on a number
of different factors.

149
00:09:14,500 --> 00:09:22,960
One is that the algorithms
reflect a severe racial bias.

150
00:09:22,960 --> 00:09:28,620
So for example, if you are two
identical people but one of you

151
00:09:28,620 --> 00:09:32,680
happens to be white and one
of you happens to be black,

152
00:09:32,680 --> 00:09:34,740
the chances of you
getting bail are

153
00:09:34,740 --> 00:09:39,170
much lower if you're black
than if you're white.

154
00:09:39,170 --> 00:09:41,180
Now, you say, well,
how could that

155
00:09:41,180 --> 00:09:44,450
be given that we're learning
this algorithmically?

156
00:09:44,450 --> 00:09:46,970
Well, it's a complicated
feedback loop

157
00:09:46,970 --> 00:09:51,290
because the algorithm is
learning from historical data,

158
00:09:51,290 --> 00:09:56,420
and if historically, judges have
been less likely to grant bail

159
00:09:56,420 --> 00:10:01,730
to an African-American than
to a Caucasian-American,

160
00:10:01,730 --> 00:10:04,970
then the algorithm will learn
that that's the right thing

161
00:10:04,970 --> 00:10:07,850
to do and will
nicely incorporate

162
00:10:07,850 --> 00:10:11,030
exactly that prejudice.

163
00:10:11,030 --> 00:10:13,040
And then the second
problem, which

164
00:10:13,040 --> 00:10:15,890
I consider to be
really horrendous,

165
00:10:15,890 --> 00:10:18,860
is that in this
particular field,

166
00:10:18,860 --> 00:10:21,230
the algorithms are
developed privately

167
00:10:21,230 --> 00:10:24,980
by private companies
which will not tell you

168
00:10:24,980 --> 00:10:27,560
what their algorithm is.

169
00:10:27,560 --> 00:10:31,890
You can just pay them and
they will tell you the answer,

170
00:10:31,890 --> 00:10:33,800
but they won't tell you
how they compute it.

171
00:10:33,800 --> 00:10:35,390
They won't tell
you what data they

172
00:10:35,390 --> 00:10:37,740
used to train the algorithm.

173
00:10:37,740 --> 00:10:39,725
And so it's really a black box.

174
00:10:39,725 --> 00:10:43,190
And so you have no idea
what's going on in that box

175
00:10:43,190 --> 00:10:45,830
other than by looking
at its decisions.

176
00:10:45,830 --> 00:10:49,690

177
00:10:49,690 --> 00:10:51,600
And so the data
collection system

178
00:10:51,600 --> 00:10:54,810
is flawed in the same way as
the judicial system itself.

179
00:10:54,810 --> 00:10:59,320

180
00:10:59,320 --> 00:11:03,030
So not only are
there algorithms that

181
00:11:03,030 --> 00:11:04,890
decide whether you
get bail or not,

182
00:11:04,890 --> 00:11:08,220
which is after all a
relatively temporary question

183
00:11:08,220 --> 00:11:10,110
until your trial comes
up, although that

184
00:11:10,110 --> 00:11:12,660
may be a long time,
but there are also

185
00:11:12,660 --> 00:11:17,140
algorithms that advise on
things like sentencing.

186
00:11:17,140 --> 00:11:21,150
So they say, how likely is this
patient to be a recidivist?

187
00:11:21,150 --> 00:11:23,610
Somebody who, when
they get out of jail,

188
00:11:23,610 --> 00:11:25,650
they're going to offend again.

189
00:11:25,650 --> 00:11:28,083
And therefore, they deserve
a longer jail sentence

190
00:11:28,083 --> 00:11:30,000
because you want to keep
them off the streets.

191
00:11:30,000 --> 00:11:34,440

192
00:11:34,440 --> 00:11:38,730
Well, so this is a particular
story about a particular person

193
00:11:38,730 --> 00:11:43,980
in Wisconsin, and shockingly,
the state Supreme Court

194
00:11:43,980 --> 00:11:47,010
ruled against this guy,
saying that knowledge

195
00:11:47,010 --> 00:11:50,610
of the algorithm's output
was a sufficient level

196
00:11:50,610 --> 00:11:55,560
of transparency in order
to not violate his rights,

197
00:11:55,560 --> 00:11:58,140
which I think many
people consider to be

198
00:11:58,140 --> 00:12:01,230
kind of an outrageous decision.

199
00:12:01,230 --> 00:12:05,280
I'm sure it'll be appealed
and maybe overturned.

200
00:12:05,280 --> 00:12:11,070
Conversely-- I keep doing on
the one hand and on the other--

201
00:12:11,070 --> 00:12:13,820
algorithms could help
keep people out of jail.

202
00:12:13,820 --> 00:12:18,450
So there's a Wired
article not long ago

203
00:12:18,450 --> 00:12:26,490
that says we can use algorithms
to analyze people's cases

204
00:12:26,490 --> 00:12:30,630
and say, oh, this person
looks like they're really

205
00:12:30,630 --> 00:12:34,110
in need of psychiatric help
rather than in need of jail

206
00:12:34,110 --> 00:12:36,810
time, and so perhaps
we can divert him

207
00:12:36,810 --> 00:12:43,110
from the penal system
into psychiatric care

208
00:12:43,110 --> 00:12:47,520
and keep him out of prison
and get him help and so on.

209
00:12:47,520 --> 00:12:50,280
So that's the
positive side of being

210
00:12:50,280 --> 00:12:53,730
able to use these
kinds of algorithms.

211
00:12:53,730 --> 00:12:56,310
Now, it's not only
in criminality.

212
00:12:56,310 --> 00:12:59,170
There is also a
long discussion--

213
00:12:59,170 --> 00:13:01,470
you can find this
all over the web--

214
00:13:01,470 --> 00:13:03,660
of, for example,
can an algorithm

215
00:13:03,660 --> 00:13:06,660
hire better than a human being.

216
00:13:06,660 --> 00:13:10,340
So if you're a big company
and you have a lot of people

217
00:13:10,340 --> 00:13:13,610
that you're trying to
hire for various jobs,

218
00:13:13,610 --> 00:13:16,220
it's very tempting
to say, hey, I've

219
00:13:16,220 --> 00:13:19,160
made lots and lots
of hiring decisions

220
00:13:19,160 --> 00:13:21,050
and we have some outcome data.

221
00:13:21,050 --> 00:13:24,500
I know which people have
turned out to be good employees

222
00:13:24,500 --> 00:13:27,590
and which people have turned
out to be bad employees,

223
00:13:27,590 --> 00:13:34,400
and therefore, we can base
a first-cut screening method

224
00:13:34,400 --> 00:13:38,390
on learning such an
algorithm and using it

225
00:13:38,390 --> 00:13:43,130
on people who apply for jobs
and say, OK, these are the ones

226
00:13:43,130 --> 00:13:46,400
that we're going to interview
and maybe hire because they

227
00:13:46,400 --> 00:13:48,470
look like they're a better bet.

228
00:13:48,470 --> 00:13:51,570
Now, I have to tell
you a personal story.

229
00:13:51,570 --> 00:13:54,730
When I was an
undergraduate at Caltech,

230
00:13:54,730 --> 00:13:57,410
the Caltech faculty
decided that they

231
00:13:57,410 --> 00:14:01,010
wanted to include student
members of all the faculty

232
00:14:01,010 --> 00:14:02,240
committees.

233
00:14:02,240 --> 00:14:04,610
And so I was lucky
enough that I served

234
00:14:04,610 --> 00:14:07,760
for three years as a member of
the Undergraduate Admissions

235
00:14:07,760 --> 00:14:10,010
Committee at Caltech.

236
00:14:10,010 --> 00:14:14,550
And in those days, Caltech
only took about 220,

237
00:14:14,550 --> 00:14:16,670
230 students a year.

238
00:14:16,670 --> 00:14:18,530
It's a very small school.

239
00:14:18,530 --> 00:14:22,490
And we would actually fly
around the country and interview

240
00:14:22,490 --> 00:14:25,790
about the top half of all the
applicants in the applicant

241
00:14:25,790 --> 00:14:26,570
pool.

242
00:14:26,570 --> 00:14:28,790
So we would talk not
only to the students

243
00:14:28,790 --> 00:14:31,970
but also to their teachers
and their counselors

244
00:14:31,970 --> 00:14:34,880
and see what the
environment was like,

245
00:14:34,880 --> 00:14:38,120
and I think we got a very good
sense of how good a student was

246
00:14:38,120 --> 00:14:40,790
likely to be based on that.

247
00:14:40,790 --> 00:14:47,090
So one day, after the admissions
decisions have been made,

248
00:14:47,090 --> 00:14:52,160
one of the professors, kind
of as a thought experiment,

249
00:14:52,160 --> 00:14:55,280
said here's what we ought to do.

250
00:14:55,280 --> 00:14:58,310
We ought to take the 230
people that we've just

251
00:14:58,310 --> 00:15:01,700
offered admission to and
we should reject them all

252
00:15:01,700 --> 00:15:05,510
and take the next
230 people, and then

253
00:15:05,510 --> 00:15:08,420
see whether the faculty notices.

254
00:15:08,420 --> 00:15:12,770
Because it seemed like a
fairly flat distribution.

255
00:15:12,770 --> 00:15:16,250
Now, of course, I
and others argued

256
00:15:16,250 --> 00:15:19,010
that this would be
unfair and unethical

257
00:15:19,010 --> 00:15:22,340
and would be a waste
of all the time

258
00:15:22,340 --> 00:15:24,890
that we had put into
selecting these people,

259
00:15:24,890 --> 00:15:26,330
so we didn't do that.

260
00:15:26,330 --> 00:15:29,330
But then this guy
went out and he

261
00:15:29,330 --> 00:15:33,590
looked at the data we had
on people's ranking class,

262
00:15:33,590 --> 00:15:39,320
SAT scores, grade point
average, the checkmarks

263
00:15:39,320 --> 00:15:41,870
on their recommendation
letters about whether they

264
00:15:41,870 --> 00:15:45,050
were truly exceptional
or merely outstanding.

265
00:15:45,050 --> 00:15:48,050

266
00:15:48,050 --> 00:15:54,110
And he built a linear
regression model

267
00:15:54,110 --> 00:15:57,530
that predicted the person's
sophomore level grade point

268
00:15:57,530 --> 00:16:00,170
average, which seemed
like a reasonable thing

269
00:16:00,170 --> 00:16:02,300
to try to predict.

270
00:16:02,300 --> 00:16:04,880
And he got a
reasonably good fit,

271
00:16:04,880 --> 00:16:06,920
but what was
disturbing about it is

272
00:16:06,920 --> 00:16:14,120
that in the Caltech
population of students,

273
00:16:14,120 --> 00:16:22,190
it turned out that the beta for
your SAT English performance

274
00:16:22,190 --> 00:16:25,460
was negative.

275
00:16:25,460 --> 00:16:30,670
So if you did particularly
well in English on the SAT,

276
00:16:30,670 --> 00:16:34,600
you were likely to do worse
as a sophomore at Caltech

277
00:16:34,600 --> 00:16:37,070
than if you didn't do as well.

278
00:16:37,070 --> 00:16:39,040
And so we thought
about that a lot,

279
00:16:39,040 --> 00:16:40,900
and of course, we
decided that that

280
00:16:40,900 --> 00:16:43,780
would be really unfair
to penalize somebody

281
00:16:43,780 --> 00:16:47,440
for being good at something,
especially when the school had

282
00:16:47,440 --> 00:16:50,410
this philosophical
orientation that

283
00:16:50,410 --> 00:16:55,520
said we ought to look for
people with broad educations.

284
00:16:55,520 --> 00:16:59,110
So that's just an example.

285
00:16:59,110 --> 00:17:02,230
And more, Science
Friday had a nice show

286
00:17:02,230 --> 00:17:06,050
that you can listen
to about this issue.

287
00:17:06,050 --> 00:17:11,040
So let me ask you, what
do you mean by fairness?

288
00:17:11,040 --> 00:17:21,460
If we're going to define
the concept, what is fair?

289
00:17:21,460 --> 00:17:26,869

290
00:17:26,869 --> 00:17:28,760
What characteristics
would you like

291
00:17:28,760 --> 00:17:31,430
to have an algorithm
have that judges you

292
00:17:31,430 --> 00:17:33,990
for some particular purpose?

293
00:17:33,990 --> 00:17:34,990
Yeah?

294
00:17:34,990 --> 00:17:37,430
AUDIENCE: It's impossible to
pin down sort of, at least

295
00:17:37,430 --> 00:17:39,620
might in my opinion,
one specific definition,

296
00:17:39,620 --> 00:17:42,200
but for the pre-trial
success rate for example,

297
00:17:42,200 --> 00:17:46,010
I think having the error rates
be similar across populations,

298
00:17:46,010 --> 00:17:48,680
across the covariants you
might care about, for example,

299
00:17:48,680 --> 00:17:51,430
fairness, I think
is a good start.

300
00:17:51,430 --> 00:17:57,490

301
00:17:57,490 --> 00:18:00,940
PETER SZOLOVITS: OK, so similar
error rates is definitely

302
00:18:00,940 --> 00:18:06,400
one of the criteria that people
use in talking about fairness.

303
00:18:06,400 --> 00:18:08,880
And you'll see later Irene--

304
00:18:08,880 --> 00:18:09,640
where's Irene?

305
00:18:09,640 --> 00:18:11,050
Right there.

306
00:18:11,050 --> 00:18:15,265
Irene is a master of
that notion of fairness.

307
00:18:15,265 --> 00:18:17,960

308
00:18:17,960 --> 00:18:18,460
Yeah?

309
00:18:18,460 --> 00:18:22,030
AUDIENCE: When the model says
some sort of observation that

310
00:18:22,030 --> 00:18:24,010
causally shouldn't
be true, and what

311
00:18:24,010 --> 00:18:28,240
I want society to look like

312
00:18:28,240 --> 00:18:30,430
PETER SZOLOVITS:
So I'm not sure how

313
00:18:30,430 --> 00:18:32,170
to capture that
in a short phrase.

314
00:18:32,170 --> 00:18:36,730

315
00:18:36,730 --> 00:18:40,060
Societal goals.

316
00:18:40,060 --> 00:18:43,490

317
00:18:43,490 --> 00:18:45,230
But that's tricky, right?

318
00:18:45,230 --> 00:18:48,830
I mean, suppose that I
would like it to be the case

319
00:18:48,830 --> 00:18:52,400
that the fraction of people
of different ethnicity

320
00:18:52,400 --> 00:18:56,650
who are criminals
should be the same.

321
00:18:56,650 --> 00:18:58,915
That seems like a good
goal for fairness.

322
00:18:58,915 --> 00:19:02,570

323
00:19:02,570 --> 00:19:04,420
How do I achieve that?

324
00:19:04,420 --> 00:19:07,120
I mean, I could pretend
that it's the same,

325
00:19:07,120 --> 00:19:10,600
but it isn't the same
today objectively,

326
00:19:10,600 --> 00:19:12,640
and the data wouldn't
support that.

327
00:19:12,640 --> 00:19:15,270
So that's an issue.

328
00:19:15,270 --> 00:19:17,070
Yeah?

329
00:19:17,070 --> 00:19:20,305
AUDIENCE: People who are similar
should be treated similarly,

330
00:19:20,305 --> 00:19:25,540
so engaged sort of
independent of the [INAUDIBLE]

331
00:19:25,540 --> 00:19:29,070
attributes or independent
of your covariate.

332
00:19:29,070 --> 00:19:31,020
PETER SZOLOVITS:
Similar people should

333
00:19:31,020 --> 00:19:36,240
lead to similar treatment.

334
00:19:36,240 --> 00:19:39,450
Yeah, I like that.

335
00:19:39,450 --> 00:19:41,770
AUDIENCE: I didn't make it up.

336
00:19:41,770 --> 00:19:43,530
PETER SZOLOVITS: I know.

337
00:19:43,530 --> 00:19:47,230
It's another of the classic
sort of notions of fairness.

338
00:19:47,230 --> 00:19:50,710

339
00:19:50,710 --> 00:19:56,350
That puts a lot of weight on
the distance function, right?

340
00:19:56,350 --> 00:19:59,670
In what way are
to people similar?

341
00:19:59,670 --> 00:20:01,650
And what characteristics--
you obviously

342
00:20:01,650 --> 00:20:04,780
don't want to use the
sensitive characteristics,

343
00:20:04,780 --> 00:20:08,700
the forbidden characteristics
in order to decide similarity,

344
00:20:08,700 --> 00:20:11,940
because then people will
be dissimilar in ways

345
00:20:11,940 --> 00:20:15,480
that you don't want, but
defining that function

346
00:20:15,480 --> 00:20:19,080
is a challenge.

347
00:20:19,080 --> 00:20:25,410
All right, well, let me show
you a more technical approach

348
00:20:25,410 --> 00:20:27,630
to thinking about this.

349
00:20:27,630 --> 00:20:31,470
So we all know about biases like
selection bias, sampling bias,

350
00:20:31,470 --> 00:20:33,480
reporting bias, et cetera.

351
00:20:33,480 --> 00:20:37,410
These are in the conventional
sense of the term bias.

352
00:20:37,410 --> 00:20:41,790
But I'll show you an example
that I got involved in.

353
00:20:41,790 --> 00:20:50,460
Raj Manrai was a MIT
Harvard HST student,

354
00:20:50,460 --> 00:20:57,240
and he started looking at the
question of the genetics that

355
00:20:57,240 --> 00:21:00,960
was used in order to
determine whether somebody

356
00:21:00,960 --> 00:21:07,170
is at risk for cardiomyopathy,
hypertrophic cardiomyopathy.

357
00:21:07,170 --> 00:21:08,080
That's a big word.

358
00:21:08,080 --> 00:21:10,620
It means that your
heart gets too big

359
00:21:10,620 --> 00:21:15,480
and it becomes sort of flabby
and it stops pumping well,

360
00:21:15,480 --> 00:21:19,080
and eventually, you die of this
disease at a relatively young

361
00:21:19,080 --> 00:21:20,670
age, if, in fact, you have it.

362
00:21:20,670 --> 00:21:24,810

363
00:21:24,810 --> 00:21:29,890
So what happened
is that there was

364
00:21:29,890 --> 00:21:34,920
a study that was done mostly
with European populations

365
00:21:34,920 --> 00:21:38,910
where they discovered that a lot
of people who had this disease

366
00:21:38,910 --> 00:21:42,130
had a certain genetic variant.

367
00:21:42,130 --> 00:21:46,400
And they said, well, that must
be the cause of this disease,

368
00:21:46,400 --> 00:21:49,720
and so it became accepted
wisdom that if you

369
00:21:49,720 --> 00:21:55,540
had that genetic variant, people
would counsel you to not plan

370
00:21:55,540 --> 00:21:57,010
on living a long life.

371
00:21:57,010 --> 00:22:01,420
And this has all
kinds of consequences.

372
00:22:01,420 --> 00:22:03,700
Imagine if you're
thinking about having

373
00:22:03,700 --> 00:22:06,850
a kid when you're
in your early 40s,

374
00:22:06,850 --> 00:22:10,000
and your life expectancy is 55.

375
00:22:10,000 --> 00:22:12,370
Would you want to die
when you have a teenager

376
00:22:12,370 --> 00:22:14,300
that you leave to your spouse?

377
00:22:14,300 --> 00:22:16,720
So this was a consequential
set of decisions

378
00:22:16,720 --> 00:22:20,080
that people have to make.

379
00:22:20,080 --> 00:22:23,800
Now, what happened
is that in the US,

380
00:22:23,800 --> 00:22:27,850
there were tests
of this sort done,

381
00:22:27,850 --> 00:22:32,290
but the problem was that a lot
of African and African-American

382
00:22:32,290 --> 00:22:37,810
populations turned out to have
this genetic variant frequently

383
00:22:37,810 --> 00:22:41,050
without developing
this terrible disease,

384
00:22:41,050 --> 00:22:46,730
but they were all told that they
were going to die, basically.

385
00:22:46,730 --> 00:22:49,810
And it was only after
years when people

386
00:22:49,810 --> 00:22:51,910
noticed that these
people who were supposed

387
00:22:51,910 --> 00:22:57,220
to die genetically weren't
dying that they said,

388
00:22:57,220 --> 00:23:00,520
maybe we misunderstood
something.

389
00:23:00,520 --> 00:23:04,090
And what they misunderstood
was that the population that

390
00:23:04,090 --> 00:23:09,550
was used to develop the
model was a European ancestry

391
00:23:09,550 --> 00:23:14,620
population and not an
African ancestry population.

392
00:23:14,620 --> 00:23:16,870
So you go, well, we must
have learned that lesson.

393
00:23:16,870 --> 00:23:21,160
So this paper was
published in 2016,

394
00:23:21,160 --> 00:23:25,380
and this was one of
the first in this area.

395
00:23:25,380 --> 00:23:26,950
Here's a paper
that was published

396
00:23:26,950 --> 00:23:32,860
three weeks ago in Nature
Scientific Reports that says,

397
00:23:32,860 --> 00:23:35,830
genetic risk factors
identified in populations

398
00:23:35,830 --> 00:23:39,580
of European descent do
not improve the prediction

399
00:23:39,580 --> 00:23:43,420
of osteoporotic fracture
and bone mineral density

400
00:23:43,420 --> 00:23:45,410
in Chinese populations.

401
00:23:45,410 --> 00:23:47,180
So it's the same story.

402
00:23:47,180 --> 00:23:48,940
It's exactly the same story.

403
00:23:48,940 --> 00:23:51,070
Different disease,
the consequence

404
00:23:51,070 --> 00:23:54,070
is probably less
dire because being

405
00:23:54,070 --> 00:23:57,040
told that you're going to break
your bones when you're old

406
00:23:57,040 --> 00:23:59,590
is not as bad as being told
that your heart's going

407
00:23:59,590 --> 00:24:05,590
to stop working when you're in
your 50s, but there we have it.

408
00:24:05,590 --> 00:24:10,300
OK, so technically, where
does bias come from?

409
00:24:10,300 --> 00:24:13,580
Well, I mentioned
the standard sources,

410
00:24:13,580 --> 00:24:15,520
but here is an
interesting analysis.

411
00:24:15,520 --> 00:24:18,670
This comes from
Constantine Aliferis

412
00:24:18,670 --> 00:24:22,540
from a number of
years ago, 2006,

413
00:24:22,540 --> 00:24:29,550
and he says, well, look,
in a perfect world,

414
00:24:29,550 --> 00:24:32,180
if I give you a
data set, there's

415
00:24:32,180 --> 00:24:35,240
an uncountably infinite
number of models

416
00:24:35,240 --> 00:24:39,900
that might possibly explain
the relationships in that data.

417
00:24:39,900 --> 00:24:42,670

418
00:24:42,670 --> 00:24:47,710
I cannot enumerate an
uncountable number of models,

419
00:24:47,710 --> 00:24:51,340
and so what I'm going to do is
choose some family of models

420
00:24:51,340 --> 00:24:55,450
to try to fit, and then I'm
going to use some fitting

421
00:24:55,450 --> 00:25:00,400
technique, like stochastic
gradient descent or something,

422
00:25:00,400 --> 00:25:05,440
that will find maybe a global
optimum, but maybe not.

423
00:25:05,440 --> 00:25:08,890
Maybe it'll find
the local optimum.

424
00:25:08,890 --> 00:25:11,370
And then there is noise.

425
00:25:11,370 --> 00:25:15,150
And so his observation
is that if you

426
00:25:15,150 --> 00:25:20,610
count O as the optimal possible
model over all possible model

427
00:25:20,610 --> 00:25:25,140
families, and if you count
L as the best model that's

428
00:25:25,140 --> 00:25:29,580
learnable by a particular
learning mechanism,

429
00:25:29,580 --> 00:25:34,800
and you call A the actual
model that's learned,

430
00:25:34,800 --> 00:25:38,520
then the bias is
essentially O minus L,

431
00:25:38,520 --> 00:25:41,910
so its limitation
of learning method

432
00:25:41,910 --> 00:25:45,300
related to the target model.

433
00:25:45,300 --> 00:25:49,140
The variance is
like L minus A, it's

434
00:25:49,140 --> 00:25:52,920
the error that's due to the
particular way in which you

435
00:25:52,920 --> 00:25:57,550
learned things, like
sampling and so on,

436
00:25:57,550 --> 00:26:02,130
and you can estimate the
significance of differences

437
00:26:02,130 --> 00:26:05,720
between different models
by just permuting the data,

438
00:26:05,720 --> 00:26:09,480
randomizing, essentially, the
relationships in the data.

439
00:26:09,480 --> 00:26:13,740
And then you get a curve of
performance of those models,

440
00:26:13,740 --> 00:26:19,180
and if yours lies outside
the 95% confidence interval,

441
00:26:19,180 --> 00:26:22,710
then you have a P
equal 0.05 result

442
00:26:22,710 --> 00:26:26,950
that this model is not random.

443
00:26:26,950 --> 00:26:29,550
So that's the typical
way of going about this.

444
00:26:29,550 --> 00:26:34,170

445
00:26:34,170 --> 00:26:42,020
Now, you might say, but isn't
discrimination the very reason

446
00:26:42,020 --> 00:26:45,080
we do machine learning?

447
00:26:45,080 --> 00:26:47,270
Not discrimination
in the legal sense,

448
00:26:47,270 --> 00:26:50,210
but discrimination in
the sense of separating

449
00:26:50,210 --> 00:26:52,230
different populations.

450
00:26:52,230 --> 00:26:55,610
And so you could say,
well, yes, but some basis

451
00:26:55,610 --> 00:26:58,460
for differentiation
are justified

452
00:26:58,460 --> 00:27:00,560
and some basis for
differentiation

453
00:27:00,560 --> 00:27:02,750
are not justified.

454
00:27:02,750 --> 00:27:05,840
So they're either
practically irrelevant,

455
00:27:05,840 --> 00:27:10,760
or we decide for
societal goals that we

456
00:27:10,760 --> 00:27:12,950
want them to be irrelevant
and we're not going

457
00:27:12,950 --> 00:27:16,430
to take them into account.

458
00:27:16,430 --> 00:27:21,470
So one lesson from people who
have studied this for a while

459
00:27:21,470 --> 00:27:24,950
is that discrimination
is domain specific.

460
00:27:24,950 --> 00:27:29,810
So you can't define
a universal notion

461
00:27:29,810 --> 00:27:33,890
of what it means to discriminate
because it's very much tied

462
00:27:33,890 --> 00:27:36,860
to these questions of
what is practically

463
00:27:36,860 --> 00:27:41,120
and morally irrelevant in the
decisions that you're making.

464
00:27:41,120 --> 00:27:43,830
And so it's going to be
different in criminal law

465
00:27:43,830 --> 00:27:47,300
than it is in medicine, than
it is in hiring, than it

466
00:27:47,300 --> 00:27:50,660
is in various other
fields, college admissions,

467
00:27:50,660 --> 00:27:52,460
for example.

468
00:27:52,460 --> 00:27:54,740
And it's
feature-specific as well,

469
00:27:54,740 --> 00:27:57,200
so you have to take
the individual features

470
00:27:57,200 --> 00:28:00,170
into account.

471
00:28:00,170 --> 00:28:02,420
Well, historically,
the government

472
00:28:02,420 --> 00:28:06,290
has tried to regulate
these domains,

473
00:28:06,290 --> 00:28:11,480
and so credit is regulated by
the Equal Credit Opportunity

474
00:28:11,480 --> 00:28:14,750
Act, education by
the Civil Rights

475
00:28:14,750 --> 00:28:18,860
Act and various amendments,
employment by the Civil Rights

476
00:28:18,860 --> 00:28:21,590
Act, housing by the
Fair Housing Act,

477
00:28:21,590 --> 00:28:24,980
public accommodation by
the Civil Rights Act,

478
00:28:24,980 --> 00:28:30,110
more recently, marriage
is regulated originally

479
00:28:30,110 --> 00:28:32,600
by the Defense of
Marriage Act, which

480
00:28:32,600 --> 00:28:34,790
as you might tell
from its title,

481
00:28:34,790 --> 00:28:39,170
was against things like
people being able to marry who

482
00:28:39,170 --> 00:28:45,620
were not a traditional marriage
that they wanted to defend,

483
00:28:45,620 --> 00:28:49,160
but it was struck down
by the Supreme Court

484
00:28:49,160 --> 00:28:53,990
about six years ago as
being discriminatory.

485
00:28:53,990 --> 00:28:57,560
It's interesting, if you look
back to probably before you

486
00:28:57,560 --> 00:29:02,400
guys were born in
1967, until 1967,

487
00:29:02,400 --> 00:29:07,010
it was illegal for an
African-American and a white

488
00:29:07,010 --> 00:29:10,140
to marry each other in Virginia.

489
00:29:10,140 --> 00:29:11,850
It was literally illegal.

490
00:29:11,850 --> 00:29:17,100
If you went to get a marriage
license, you were denied,

491
00:29:17,100 --> 00:29:19,920
and if you got married out
of state and came back,

492
00:29:19,920 --> 00:29:22,620
you could be arrested.

493
00:29:22,620 --> 00:29:24,720
This happened much later.

494
00:29:24,720 --> 00:29:28,440
Trevor Noah, if you know
him from The Daily Show,

495
00:29:28,440 --> 00:29:31,680
wrote a book called
Born a Crime,

496
00:29:31,680 --> 00:29:35,550
I think, and his father
is white Swiss guy

497
00:29:35,550 --> 00:29:39,780
and his mother is a
South African black,

498
00:29:39,780 --> 00:29:43,110
and so it was literally
illegal for him

499
00:29:43,110 --> 00:29:48,270
to exist under the apartheid
laws that they had.

500
00:29:48,270 --> 00:29:51,780
He had to pretend to be--

501
00:29:51,780 --> 00:29:56,490
his mother was his caretaker
rather than his mother

502
00:29:56,490 --> 00:29:58,650
in order to be able
to go out in public,

503
00:29:58,650 --> 00:30:02,070
because otherwise, they
would get arrested.

504
00:30:02,070 --> 00:30:05,220
So this has recently, of
course, also disappeared,

505
00:30:05,220 --> 00:30:10,430
but these are some of
the regulatory issues.

506
00:30:10,430 --> 00:30:15,130
So here are some of the legally
recognized protected classes,

507
00:30:15,130 --> 00:30:18,880
race, color, sex, religion,
national origin, citizenship,

508
00:30:18,880 --> 00:30:23,570
age, pregnancy, familial status,
disability, veteran status,

509
00:30:23,570 --> 00:30:27,160
and more recently,
sexual orientation

510
00:30:27,160 --> 00:30:30,610
in certain jurisdictions,
but not everywhere

511
00:30:30,610 --> 00:30:31,480
around the country.

512
00:30:31,480 --> 00:30:34,880

513
00:30:34,880 --> 00:30:39,320
OK, so given those
examples, there

514
00:30:39,320 --> 00:30:44,330
are two legal doctrines about
discrimination, and one of them

515
00:30:44,330 --> 00:30:47,240
talks about disparate
treatment, which

516
00:30:47,240 --> 00:30:52,540
is sort of related to this one.

517
00:30:52,540 --> 00:30:55,500
And the other talks
about disparate impact

518
00:30:55,500 --> 00:30:58,500
and says, no matter
what the mechanism

519
00:30:58,500 --> 00:31:02,670
is, if the outcome is very
different for different racial

520
00:31:02,670 --> 00:31:07,030
groups typically or
gender groups, then there

521
00:31:07,030 --> 00:31:10,700
is prima facie evidence that
there is something not right,

522
00:31:10,700 --> 00:31:14,710
that there is some
sort of discrimination.

523
00:31:14,710 --> 00:31:21,790
Now, the problem is, how do
you defend yourself against,

524
00:31:21,790 --> 00:31:24,510
for example, a disparate
impact argument?

525
00:31:24,510 --> 00:31:31,590
Well, you say, in order to
be disparate impact that's

526
00:31:31,590 --> 00:31:36,310
illegal, it has to be
unjustified or avoidable.

527
00:31:36,310 --> 00:31:39,240
So for example,
suppose I'm trying

528
00:31:39,240 --> 00:31:45,330
to hire people to climb
50-story buildings that

529
00:31:45,330 --> 00:31:49,410
are under construction, and
you apply, but it turns out

530
00:31:49,410 --> 00:31:51,450
you have a medical
condition which

531
00:31:51,450 --> 00:31:55,610
is that you get
dizzy at times, I

532
00:31:55,610 --> 00:31:59,180
might say, you know what,
I don't want to hire you,

533
00:31:59,180 --> 00:32:01,070
because I don't want
you plopping off

534
00:32:01,070 --> 00:32:05,060
the 50th floor of a building
that's under construction,

535
00:32:05,060 --> 00:32:07,880
and that's probably
a reasonable defense.

536
00:32:07,880 --> 00:32:10,530
If I brought suit
against you and said,

537
00:32:10,530 --> 00:32:12,920
hey, you're
discriminating against me

538
00:32:12,920 --> 00:32:16,430
on the basis of this
medical disability,

539
00:32:16,430 --> 00:32:21,050
a perfectly good defense
is, yeah, it's true,

540
00:32:21,050 --> 00:32:24,030
but it's relevant to the job.

541
00:32:24,030 --> 00:32:26,780
So that's one way
of dealing with it.

542
00:32:26,780 --> 00:32:30,170
Now, how do you demonstrate
disparate impact?

543
00:32:30,170 --> 00:32:32,510
Well, the court has
decided that you

544
00:32:32,510 --> 00:32:36,230
need to be able to show
about a 20% difference

545
00:32:36,230 --> 00:32:38,600
in order to call something
disparate impact.

546
00:32:38,600 --> 00:32:44,110

547
00:32:44,110 --> 00:32:48,120
So the question, of course,
is can we change our hiring

548
00:32:48,120 --> 00:32:51,840
policies or whatever
policies we're using in order

549
00:32:51,840 --> 00:32:54,930
to achieve the same
goals, but with less

550
00:32:54,930 --> 00:32:58,140
of a disparity in the impact.

551
00:32:58,140 --> 00:32:59,310
So that's the challenge.

552
00:32:59,310 --> 00:33:03,220

553
00:33:03,220 --> 00:33:06,640
Now, what's interesting is
that disparate treatment

554
00:33:06,640 --> 00:33:12,490
and disparate impact are really
in conflict with each other.

555
00:33:12,490 --> 00:33:15,280
And you'll find that
this is true in almost

556
00:33:15,280 --> 00:33:17,930
everything in this domain.

557
00:33:17,930 --> 00:33:22,330
So disparate impact is
about distributive justice

558
00:33:22,330 --> 00:33:25,240
and minimizing
equality of outcome.

559
00:33:25,240 --> 00:33:28,120
Disparate treatment is
about procedural fairness

560
00:33:28,120 --> 00:33:32,170
and equality of opportunity,
and those don't always mesh.

561
00:33:32,170 --> 00:33:36,790
In other words, it may well be
that equality of opportunity

562
00:33:36,790 --> 00:33:40,930
still leads to
differences in outcome,

563
00:33:40,930 --> 00:33:43,525
and you can't square
that circle easily.

564
00:33:43,525 --> 00:33:50,670

565
00:33:50,670 --> 00:33:53,520
Well, there's a lot
of discrimination

566
00:33:53,520 --> 00:33:54,750
that keeps persisting.

567
00:33:54,750 --> 00:33:57,900
There's plenty of evidence
in the literature.

568
00:33:57,900 --> 00:34:03,400
And one of the problems
is that, for example,

569
00:34:03,400 --> 00:34:11,610
take an issue like the disparity
between different races

570
00:34:11,610 --> 00:34:13,650
or different ethnicities.

571
00:34:13,650 --> 00:34:16,650
It turns out that we don't
have a nicely balanced

572
00:34:16,650 --> 00:34:21,870
set where the number of
people of European descent

573
00:34:21,870 --> 00:34:25,560
is equal to the number of
people of African-American,

574
00:34:25,560 --> 00:34:27,750
or Hispanic, or
Asian, or whatever

575
00:34:27,750 --> 00:34:31,380
population you choose
descent, and therefore, we

576
00:34:31,380 --> 00:34:35,250
tend to know a lot more
about the majority class

577
00:34:35,250 --> 00:34:38,130
than we know about
these minority classes,

578
00:34:38,130 --> 00:34:41,820
and just that additional data
and that additional knowledge

579
00:34:41,820 --> 00:34:46,110
might mean that we're able to
reduce the error rate simply

580
00:34:46,110 --> 00:34:49,980
because we have a
larger sample size.

581
00:34:49,980 --> 00:34:53,790
OK, so if you want
to formalize this,

582
00:34:53,790 --> 00:34:59,130
this is Moritz Hardt's
part of the tutorial

583
00:34:59,130 --> 00:35:03,420
that I'm stealing
from in this talk.

584
00:35:03,420 --> 00:35:06,060

585
00:35:06,060 --> 00:35:11,430
This was given at KDD about a
year and a half ago, I think.

586
00:35:11,430 --> 00:35:14,610
And Moritz is a
professor at Berkeley

587
00:35:14,610 --> 00:35:18,150
who actually teaches an
entire semester-long course

588
00:35:18,150 --> 00:35:21,840
on fairness in machine learning,
so there's a lot of material

589
00:35:21,840 --> 00:35:23,190
here.

590
00:35:23,190 --> 00:35:26,470
And so he formalizes
the problem this way.

591
00:35:26,470 --> 00:35:33,920
He says, look, a decision
problem, a model, in our terms,

592
00:35:33,920 --> 00:35:37,220
is that we have some X,
which is the set of features

593
00:35:37,220 --> 00:35:41,960
we know about an individual,
and we have some said A, which

594
00:35:41,960 --> 00:35:44,390
is the set of
protected features,

595
00:35:44,390 --> 00:35:48,050
like your race, or your
gender, or your age,

596
00:35:48,050 --> 00:35:51,800
or whatever it is we're trying
to prevent from discriminating

597
00:35:51,800 --> 00:35:56,210
on, and then we have either
a classifier or some score

598
00:35:56,210 --> 00:36:02,090
or predictive function
that's a function of X and A

599
00:36:02,090 --> 00:36:06,290
in either case, and then we have
some Y, which is the outcome

600
00:36:06,290 --> 00:36:09,530
that we're interested
in predicting.

601
00:36:09,530 --> 00:36:13,970
So now you can begin to tease
apart some different notions

602
00:36:13,970 --> 00:36:17,210
of fairness by looking
at the relationships

603
00:36:17,210 --> 00:36:19,520
between these elements.

604
00:36:19,520 --> 00:36:23,520
So there are three criteria
that appear in the literature.

605
00:36:23,520 --> 00:36:25,670
One of them is the
notion of independence

606
00:36:25,670 --> 00:36:29,270
of the scoring function
from sensitive attributes.

607
00:36:29,270 --> 00:36:34,430
So this says that R is
independent from A. Remember,

608
00:36:34,430 --> 00:36:40,100
on the previous slide, I said
that R is a function of--

609
00:36:40,100 --> 00:36:41,380
oops.

610
00:36:41,380 --> 00:36:46,760
R is a function of X and A,
so obviously, that criterion

611
00:36:46,760 --> 00:36:53,530
says that it can't be a
function of A. Null function.

612
00:36:53,530 --> 00:36:56,470
Another notion is
separation of score

613
00:36:56,470 --> 00:37:00,050
and the sensitive attribute
given the outcome.

614
00:37:00,050 --> 00:37:03,580
So this is the one that says
the different groups are

615
00:37:03,580 --> 00:37:05,710
going to be treated similarly.

616
00:37:05,710 --> 00:37:11,530
In other words, if I tell
you the group, the outcome,

617
00:37:11,530 --> 00:37:13,390
the people who did
well at the job

618
00:37:13,390 --> 00:37:16,420
and the people who
did poorly at the job,

619
00:37:16,420 --> 00:37:21,850
then the scoring function is
independent of the protected

620
00:37:21,850 --> 00:37:24,660
attribute.

621
00:37:24,660 --> 00:37:27,660
So that allows a
little more wiggle room

622
00:37:27,660 --> 00:37:30,810
because it says that the
protected attribute can still

623
00:37:30,810 --> 00:37:34,120
predict something
about the outcome,

624
00:37:34,120 --> 00:37:38,940
it's just that you can't use it
in the scoring function given

625
00:37:38,940 --> 00:37:41,700
the category of which
outcome category

626
00:37:41,700 --> 00:37:43,830
that individual belongs to.

627
00:37:43,830 --> 00:37:46,980
And then sufficiency
is the inverse of that.

628
00:37:46,980 --> 00:37:50,370
It says that given
the scoring function,

629
00:37:50,370 --> 00:37:54,420
the outcome is independent
of the protected attribute.

630
00:37:54,420 --> 00:37:57,690
So that says, can we
build a fair scoring

631
00:37:57,690 --> 00:38:02,280
function that separates the
outcome from the protected

632
00:38:02,280 --> 00:38:04,640
attribute?

633
00:38:04,640 --> 00:38:06,470
So here's some detail on those.

634
00:38:06,470 --> 00:38:08,750
If you look at independence--

635
00:38:08,750 --> 00:38:12,380
this is also called by
various other names--

636
00:38:12,380 --> 00:38:16,040
basically, what it says
is that the probability

637
00:38:16,040 --> 00:38:20,150
of a particular
result, R equal 1,

638
00:38:20,150 --> 00:38:23,180
is the same whether
you're in class A or class

639
00:38:23,180 --> 00:38:28,880
B in the protected attribute.

640
00:38:28,880 --> 00:38:30,020
So what does that tell you?

641
00:38:30,020 --> 00:38:32,780
That tells you that the
scoring function has

642
00:38:32,780 --> 00:38:38,080
to be universal over
the entire data set

643
00:38:38,080 --> 00:38:42,805
and has to not distinguish
between people in class

644
00:38:42,805 --> 00:38:49,920
A versus class B. That's a
pretty strong requirement.

645
00:38:49,920 --> 00:38:55,770
And then you can operationalize
the notion of unfairness

646
00:38:55,770 --> 00:38:58,740
either by looking for
an absolute difference

647
00:38:58,740 --> 00:39:00,930
between those probabilities.

648
00:39:00,930 --> 00:39:03,310
If it's greater
than some epsilon,

649
00:39:03,310 --> 00:39:07,380
then you have evidence that this
is not a fair scoring function,

650
00:39:07,380 --> 00:39:12,580
or a ratio test that says,
we look at the ratio,

651
00:39:12,580 --> 00:39:16,020
and if it differs
from 1 significantly,

652
00:39:16,020 --> 00:39:20,140
then you have evidence that this
is an unfair scoring function.

653
00:39:20,140 --> 00:39:23,130
And by the way, this
relates to the 4/5 rule,

654
00:39:23,130 --> 00:39:29,130
because if you make
epsilon 20%, then that's

655
00:39:29,130 --> 00:39:32,460
the same as the 4/5 rule.

656
00:39:32,460 --> 00:39:36,300
Now, the problem-- there are
problems with this notion

657
00:39:36,300 --> 00:39:37,900
of independence.

658
00:39:37,900 --> 00:39:42,090
So it only requires
equal rates of decisions

659
00:39:42,090 --> 00:39:45,990
for hiring, or giving somebody
a liver for transplant,

660
00:39:45,990 --> 00:39:49,840
or whatever topic
you're interested in.

661
00:39:49,840 --> 00:39:54,900
And so what if hiring is based
on a good score in group A,

662
00:39:54,900 --> 00:39:57,060
but random in B?

663
00:39:57,060 --> 00:40:01,860
So for example, what if we
know a lot more information

664
00:40:01,860 --> 00:40:05,220
about group A than
we do about group B,

665
00:40:05,220 --> 00:40:08,000
so we have a better
way of scoring them

666
00:40:08,000 --> 00:40:12,480
than we do of scoring group
B. So you might wind up

667
00:40:12,480 --> 00:40:15,630
with a situation
where you wind up

668
00:40:15,630 --> 00:40:18,290
hiring the same
number of people,

669
00:40:18,290 --> 00:40:22,800
the same ratio of people in
both groups, but in one group,

670
00:40:22,800 --> 00:40:24,945
you've done a good
job of selecting out

671
00:40:24,945 --> 00:40:27,510
the good candidates,
and in the other group,

672
00:40:27,510 --> 00:40:31,290
you've essentially
done it at random.

673
00:40:31,290 --> 00:40:35,130
Well, the outcomes are likely
to be better for a group A

674
00:40:35,130 --> 00:40:38,940
than for group B, which means
that you're developing more

675
00:40:38,940 --> 00:40:42,090
data for the future
that says, we really

676
00:40:42,090 --> 00:40:44,430
ought to be hiring
people in group A

677
00:40:44,430 --> 00:40:47,200
because they have
better outcomes.

678
00:40:47,200 --> 00:40:50,170
So there's this feedback loop.

679
00:40:50,170 --> 00:40:52,600
Or alternatively--
well, of course,

680
00:40:52,600 --> 00:40:54,385
it could be caused
by malice also.

681
00:40:54,385 --> 00:40:57,250

682
00:40:57,250 --> 00:41:00,310
I could just decide as
a hiring manager I'm not

683
00:41:00,310 --> 00:41:03,010
hiring enough
African-Americans so I'm just

684
00:41:03,010 --> 00:41:06,280
going to take some random
sample of African-Americans

685
00:41:06,280 --> 00:41:09,710
and hire them, and then
maybe they'll do badly,

686
00:41:09,710 --> 00:41:12,010
and then I'll have more
data to demonstrate

687
00:41:12,010 --> 00:41:14,490
that this was a bad idea.

688
00:41:14,490 --> 00:41:18,150
So that would be malicious.

689
00:41:18,150 --> 00:41:19,830
There's also a
technical problem,

690
00:41:19,830 --> 00:41:24,450
which is it's possible that
the category, the group

691
00:41:24,450 --> 00:41:27,480
is a perfect predictor
of the outcome, in which

692
00:41:27,480 --> 00:41:29,760
case, of course, they
can't be separated.

693
00:41:29,760 --> 00:41:34,820
They can't be independent
of each other.

694
00:41:34,820 --> 00:41:37,540
Now, how do you
achieve independence?

695
00:41:37,540 --> 00:41:39,920
Well, there are a number
of different techniques.

696
00:41:39,920 --> 00:41:42,400
One of them is--

697
00:41:42,400 --> 00:41:46,690
there's this article
by Zemel about learning

698
00:41:46,690 --> 00:41:50,080
fair representations,
and what it says

699
00:41:50,080 --> 00:41:55,810
is you create a new
world representation, Z,

700
00:41:55,810 --> 00:41:59,845
which is some
combination of X and A,

701
00:41:59,845 --> 00:42:04,290
and you do this by maximizing
the mutual information

702
00:42:04,290 --> 00:42:07,800
between X and Z
and by minimizing

703
00:42:07,800 --> 00:42:12,410
the mutual information
between the A and Z.

704
00:42:12,410 --> 00:42:14,360
So this is an idea
that I've seen

705
00:42:14,360 --> 00:42:20,150
used in machine learning
for robustness rather

706
00:42:20,150 --> 00:42:23,450
than for fairness,
where people say,

707
00:42:23,450 --> 00:42:26,210
the problem is that given
a particular data set,

708
00:42:26,210 --> 00:42:30,470
you can overfit to that data
set, and so one of the ideas

709
00:42:30,470 --> 00:42:34,700
is to do a Gann-like
method where you say,

710
00:42:34,700 --> 00:42:37,790
I want to train my
classifier, let's say,

711
00:42:37,790 --> 00:42:42,050
not only to work well on
getting the right answer,

712
00:42:42,050 --> 00:42:47,680
but also to work as poorly as
possible on identifying which

713
00:42:47,680 --> 00:42:51,190
data set my example came from.

714
00:42:51,190 --> 00:42:52,960
So this is the
same sort of idea.

715
00:42:52,960 --> 00:42:55,480
It's a representation
learning idea.

716
00:42:55,480 --> 00:42:58,780
And then you build
your predictor, R,

717
00:42:58,780 --> 00:43:02,890
based on this representation,
which is perhaps not

718
00:43:02,890 --> 00:43:06,940
perfectly independent of
the protected attribute,

719
00:43:06,940 --> 00:43:09,920
but is as independent
as possible.

720
00:43:09,920 --> 00:43:12,790
And usually, there are knobs
in these learning algorithms,

721
00:43:12,790 --> 00:43:15,190
and depending on how
you turn the knob,

722
00:43:15,190 --> 00:43:18,260
you can affect
whether you're going

723
00:43:18,260 --> 00:43:21,790
to get a better classifier
that's more discriminatory

724
00:43:21,790 --> 00:43:25,910
or a worse classifier
that's less discriminatory.

725
00:43:25,910 --> 00:43:29,720
So you can do that
in pre-processing.

726
00:43:29,720 --> 00:43:34,490
You can do some kind of
incorporating in the loss

727
00:43:34,490 --> 00:43:40,880
function a dependence notion or
an independence notion and say,

728
00:43:40,880 --> 00:43:44,120
we're going to train on
a particular data set,

729
00:43:44,120 --> 00:43:48,260
imposing this notion of
wanting this independence

730
00:43:48,260 --> 00:43:53,600
between A and R as
part of our desiderata.

731
00:43:53,600 --> 00:43:56,180
And so you, again,
are making trade-offs

732
00:43:56,180 --> 00:43:58,640
against other characteristics.

733
00:43:58,640 --> 00:44:00,590
Or you can do post-processing.

734
00:44:00,590 --> 00:44:05,600
So suppose I've built an
optimal R, not worrying

735
00:44:05,600 --> 00:44:09,350
about discrimination, then
I can do another learning

736
00:44:09,350 --> 00:44:13,850
problem that says I'm now going
to build a new F, which takes

737
00:44:13,850 --> 00:44:17,920
R and the protected
attribute into account,

738
00:44:17,920 --> 00:44:22,660
and it's going to minimize the
cost of misclassifications.

739
00:44:22,660 --> 00:44:26,080
And again, there's a knob where
you can say, how much do I

740
00:44:26,080 --> 00:44:28,900
want to emphasize
misclassifications

741
00:44:28,900 --> 00:44:31,390
for the protected
attribute or based

742
00:44:31,390 --> 00:44:33,225
on the protected attribute?

743
00:44:33,225 --> 00:44:36,620

744
00:44:36,620 --> 00:44:41,000
So this was still talking
about independence.

745
00:44:41,000 --> 00:44:45,890
The next notion is separation,
that says given the outcome,

746
00:44:45,890 --> 00:44:52,190
I want to separate A and R.
So that graphical model shows

747
00:44:52,190 --> 00:44:56,480
that the protected attribute
is only related to the scoring

748
00:44:56,480 --> 00:45:00,150
function through the outcome.

749
00:45:00,150 --> 00:45:04,490
So there's nothing else that you
can learn from one to the other

750
00:45:04,490 --> 00:45:07,530
than through the outcome.

751
00:45:07,530 --> 00:45:11,540
So this recognizes that
the protected attribute

752
00:45:11,540 --> 00:45:14,150
may, in fact, be correlated
with the target variable.

753
00:45:14,150 --> 00:45:16,730

754
00:45:16,730 --> 00:45:20,120
An example might be
different success rates

755
00:45:20,120 --> 00:45:24,330
in a drug trial for
different ethnic populations.

756
00:45:24,330 --> 00:45:31,940
There are now some cardiac
drugs where the manufacturer has

757
00:45:31,940 --> 00:45:35,210
determined that
this drug works much

758
00:45:35,210 --> 00:45:38,180
better in certain
subpopulations than it does

759
00:45:38,180 --> 00:45:40,760
in other populations,
and the FDA

760
00:45:40,760 --> 00:45:44,420
has actually approved the
marketing of that drug

761
00:45:44,420 --> 00:45:46,890
to those subpopulations.

762
00:45:46,890 --> 00:45:49,190
So you're not
supposed to market it

763
00:45:49,190 --> 00:45:53,690
to the people for whom
it doesn't work as well,

764
00:45:53,690 --> 00:45:56,030
but you're allowed to
market it specifically

765
00:45:56,030 --> 00:45:58,890
for the people for
whom it does work well.

766
00:45:58,890 --> 00:46:01,400
And if you think about
the personalized medicine

767
00:46:01,400 --> 00:46:06,230
idea, which we've
talked about earlier.

768
00:46:06,230 --> 00:46:08,450
The populations that
we're interested in

769
00:46:08,450 --> 00:46:12,770
becomes smaller and smaller
until it may just be you.

770
00:46:12,770 --> 00:46:16,730
And so there might be a drug
that works for you and not

771
00:46:16,730 --> 00:46:19,910
for anybody else in the
class, but it's exactly

772
00:46:19,910 --> 00:46:23,810
the right drug for you,
and we may get to the point

773
00:46:23,810 --> 00:46:26,870
where that will happen and
where we can build such drugs

774
00:46:26,870 --> 00:46:32,960
and where we can approve their
use in human populations.

775
00:46:32,960 --> 00:46:37,510
Now, the idea here
is that if I have

776
00:46:37,510 --> 00:46:44,290
two populations, blue and green,
and I draw ROC curves for both

777
00:46:44,290 --> 00:46:46,870
of these populations,
they're not

778
00:46:46,870 --> 00:46:50,500
going to be the same, because
the drug will work differently

779
00:46:50,500 --> 00:46:52,750
for those two populations.

780
00:46:52,750 --> 00:46:56,140
But on the other hand, I can
draw them on the same axes,

781
00:46:56,140 --> 00:47:01,460
and I can say, look any place
within this colored region

782
00:47:01,460 --> 00:47:04,670
can be a fair region
in that I'm going

783
00:47:04,670 --> 00:47:08,130
to get the same outcome
for both populations.

784
00:47:08,130 --> 00:47:12,220
So I can't achieve this
outcome for the blue population

785
00:47:12,220 --> 00:47:15,110
or this outcome for
the green population,

786
00:47:15,110 --> 00:47:19,160
but I can achieve any of these
outcomes for both populations

787
00:47:19,160 --> 00:47:20,880
simultaneously.

788
00:47:20,880 --> 00:47:25,370
And so that's one way of
going about satisfying

789
00:47:25,370 --> 00:47:30,200
this requirement when it
is not easily satisfied.

790
00:47:30,200 --> 00:47:33,350
So the advantage of
separation over independence

791
00:47:33,350 --> 00:47:35,720
is that it allows
correlation between R

792
00:47:35,720 --> 00:47:39,290
and Y, even a perfect
predictor, so R

793
00:47:39,290 --> 00:47:42,320
could be a perfect
predictor for Y.

794
00:47:42,320 --> 00:47:45,290
And it gives you incentives
to learn to reduce

795
00:47:45,290 --> 00:47:47,270
the errors in all groups.

796
00:47:47,270 --> 00:47:50,450
So that issue about
randomly choosing

797
00:47:50,450 --> 00:47:53,660
members of the minority
group doesn't work here

798
00:47:53,660 --> 00:47:57,890
because that would suppress
the ROC curve to the point

799
00:47:57,890 --> 00:48:01,980
where there would be no feasible
region that you would like.

800
00:48:01,980 --> 00:48:04,430
So for example, if
it's a coin flip,

801
00:48:04,430 --> 00:48:06,770
then you'd have
the diagonal line

802
00:48:06,770 --> 00:48:10,760
and the only feasible region
would be below that diagonal,

803
00:48:10,760 --> 00:48:14,490
no matter how good the predictor
was for the other class.

804
00:48:14,490 --> 00:48:17,770
So that's a nice characteristic.

805
00:48:17,770 --> 00:48:21,440
And then the final
criterion is sufficiency,

806
00:48:21,440 --> 00:48:26,000
which flips R and Y. So
it says that the regressor

807
00:48:26,000 --> 00:48:30,730
or the predictive variable can
depend on the protected class,

808
00:48:30,730 --> 00:48:34,910
but the protected class is
separated from the outcome.

809
00:48:34,910 --> 00:48:40,150
So for example, the
probability in a binary case

810
00:48:40,150 --> 00:48:45,070
of a true outcome
of Y given that R

811
00:48:45,070 --> 00:48:50,470
is some particular value, R
and A is a particular class,

812
00:48:50,470 --> 00:48:54,340
is the same as the probability
of that same outcome given

813
00:48:54,340 --> 00:49:00,740
the same R value, but
the different class.

814
00:49:00,740 --> 00:49:05,420
So that's related to the
sort of similar people,

815
00:49:05,420 --> 00:49:11,270
similar treatment notion,
qualitative notion, again.

816
00:49:11,270 --> 00:49:13,790

817
00:49:13,790 --> 00:49:17,750
So it requires a parody
of both the positive

818
00:49:17,750 --> 00:49:23,230
and the negative predictive
values across different groups.

819
00:49:23,230 --> 00:49:28,310
So that's another popular
way of looking at this.

820
00:49:28,310 --> 00:49:32,050
So for example, if the scoring
function is a probability,

821
00:49:32,050 --> 00:49:36,220
or the set of all instances
assigned the score R has an R

822
00:49:36,220 --> 00:49:39,560
fraction of positive
instances among them,

823
00:49:39,560 --> 00:49:42,550
then the scoring function is
said to be well-calibrated.

824
00:49:42,550 --> 00:49:46,000
So we've talked about
that before in the class.

825
00:49:46,000 --> 00:49:50,150
If it turns out that R
is not well-calibrated,

826
00:49:50,150 --> 00:49:54,640
you can hack it and you can make
it well-calibrated by putting

827
00:49:54,640 --> 00:49:58,570
it through a logistic function
that will then approximate

828
00:49:58,570 --> 00:50:01,990
the appropriately
calibrated score,

829
00:50:01,990 --> 00:50:07,030
and then you hope that that
calibration will give--

830
00:50:07,030 --> 00:50:10,180
or the degree of
calibration will give you

831
00:50:10,180 --> 00:50:14,320
a good approximation to
this notion of sufficiency.

832
00:50:14,320 --> 00:50:18,400
These guys in the
tutorial also point out

833
00:50:18,400 --> 00:50:24,370
that some data sets actually
lead to good calibration

834
00:50:24,370 --> 00:50:26,720
without even trying very hard.

835
00:50:26,720 --> 00:50:31,060
So for example, this is
the UCI census data set,

836
00:50:31,060 --> 00:50:33,910
and it's a binary prediction
of whether somebody makes

837
00:50:33,910 --> 00:50:38,500
more than $50,000 a year if
you have any income at all

838
00:50:38,500 --> 00:50:40,930
and if you're over 16 years old.

839
00:50:40,930 --> 00:50:45,370
And the feature, there are 14
features, age, type of work,

840
00:50:45,370 --> 00:50:47,890
weight of sample is
some statistical hack

841
00:50:47,890 --> 00:50:51,550
from the Census Bureau,
your education level,

842
00:50:51,550 --> 00:50:54,370
marital status, et
cetera, and what

843
00:50:54,370 --> 00:50:59,230
you see is that the calibration
for males and females

844
00:50:59,230 --> 00:51:00,580
is pretty decent.

845
00:51:00,580 --> 00:51:04,540
It's almost exactly
along the 45 degree line

846
00:51:04,540 --> 00:51:07,690
without having done anything
particularly dramatic

847
00:51:07,690 --> 00:51:10,000
in order to achieve that.

848
00:51:10,000 --> 00:51:13,420
On the other hand, if you
look at the calibration curve

849
00:51:13,420 --> 00:51:16,690
by race for whites
versus blacks,

850
00:51:16,690 --> 00:51:20,800
the whites, not surprisingly,
are reasonably well-calibrated,

851
00:51:20,800 --> 00:51:23,410
and the blacks are not
as well-calibrated.

852
00:51:23,410 --> 00:51:26,680
So you could imagine building
some kind of a transformation

853
00:51:26,680 --> 00:51:29,710
function to improve
that calibration,

854
00:51:29,710 --> 00:51:32,680
and that would get
you separation.

855
00:51:32,680 --> 00:51:35,570
Now, there's a
terrible piece of news,

856
00:51:35,570 --> 00:51:40,450
which is that you can prove,
as they do in this tutorial,

857
00:51:40,450 --> 00:51:43,510
that it's not possible
to jointly achieve

858
00:51:43,510 --> 00:51:46,010
any pair of these conditions.

859
00:51:46,010 --> 00:51:49,660
So you have three
reasonable technical notions

860
00:51:49,660 --> 00:51:52,060
of what fairness
means, and they're

861
00:51:52,060 --> 00:51:57,570
incompatible with each other
except in some trivial cases.

862
00:51:57,570 --> 00:51:58,710
This is not good.

863
00:51:58,710 --> 00:52:02,210

864
00:52:02,210 --> 00:52:04,600
And I'm not going to
have time to go into it,

865
00:52:04,600 --> 00:52:07,920
but there's a very
nice thing from Google

866
00:52:07,920 --> 00:52:12,960
where they illustrate
the results of adopting

867
00:52:12,960 --> 00:52:16,380
one or another of these
notions of fairness

868
00:52:16,380 --> 00:52:20,610
on a synthesized
population of people,

869
00:52:20,610 --> 00:52:23,460
and you can see how
the trade-offs vary

870
00:52:23,460 --> 00:52:25,650
and what the results
are of choosing

871
00:52:25,650 --> 00:52:27,700
different notions of fairness.

872
00:52:27,700 --> 00:52:29,940
So it's a kind of
nice graphical hack.

873
00:52:29,940 --> 00:52:31,980
Again, it'll be on
the slides, and I

874
00:52:31,980 --> 00:52:33,900
urge you to check
that out, but I'm not

875
00:52:33,900 --> 00:52:36,340
going to have time
to go into it.

876
00:52:36,340 --> 00:52:39,370
There is one other problem
that they point out

877
00:52:39,370 --> 00:52:42,410
which is interesting.

878
00:52:42,410 --> 00:52:45,520
So this was a
scenario where you're

879
00:52:45,520 --> 00:52:49,090
trying to hire
computer programmers,

880
00:52:49,090 --> 00:52:52,240
and you don't want to take
gender into account because we

881
00:52:52,240 --> 00:52:55,780
know that women are
underrepresented among computer

882
00:52:55,780 --> 00:52:58,420
people, and so we
would like that not

883
00:52:58,420 --> 00:53:00,490
to be an allowed
attribute in order

884
00:53:00,490 --> 00:53:03,110
to decide to hire someone.

885
00:53:03,110 --> 00:53:06,910
So they say, well,
there are two scenarios.

886
00:53:06,910 --> 00:53:12,100
One of them is that gender,
A, influences whether you're

887
00:53:12,100 --> 00:53:13,675
a programmer or not.

888
00:53:13,675 --> 00:53:15,850
And this is empirically true.

889
00:53:15,850 --> 00:53:19,630
There are fewer women
who are programmers.

890
00:53:19,630 --> 00:53:24,310
It turns out that visiting
Pinterest is slightly more

891
00:53:24,310 --> 00:53:27,330
common among women than men.

892
00:53:27,330 --> 00:53:30,330
Who knew?

893
00:53:30,330 --> 00:53:35,430
And then visiting GitHub is much
more common among programmers

894
00:53:35,430 --> 00:53:39,360
than among non-programmers.

895
00:53:39,360 --> 00:53:41,590
That one's pretty obvious.

896
00:53:41,590 --> 00:53:47,880
So what they say is, if you
want an optimal predictor

897
00:53:47,880 --> 00:53:51,000
of whether somebody's
going to get hired,

898
00:53:51,000 --> 00:53:54,630
it should actually take both
Pinterest visits and GitHub

899
00:53:54,630 --> 00:54:02,790
visits into account, but
because those go back

900
00:54:02,790 --> 00:54:09,860
to gender, which is
an unusable attribute,

901
00:54:09,860 --> 00:54:11,550
they don't like this model.

902
00:54:11,550 --> 00:54:18,140
And so they say, well, we could
use an optimal separated score,

903
00:54:18,140 --> 00:54:23,000
because now, being a programmer
separates your gender

904
00:54:23,000 --> 00:54:25,440
from the scoring function.

905
00:54:25,440 --> 00:54:28,010
And so we can create
a different score

906
00:54:28,010 --> 00:54:31,490
which is not the same
as the optimal score,

907
00:54:31,490 --> 00:54:39,380
but is permitted because it's
no longer dependent on your sex,

908
00:54:39,380 --> 00:54:41,580
on your gender.

909
00:54:41,580 --> 00:54:46,140
Here's another scenario that,
again, starts with gender

910
00:54:46,140 --> 00:54:51,510
and says, look, we know that
there are more men than women

911
00:54:51,510 --> 00:54:55,750
who obtain college degrees
in computer science,

912
00:54:55,750 --> 00:54:58,200
and so there's an
influence there,

913
00:54:58,200 --> 00:55:00,390
and computer scientists
are much more

914
00:55:00,390 --> 00:55:03,885
likely to be programmers than
non-computer science majors.

915
00:55:03,885 --> 00:55:07,870

916
00:55:07,870 --> 00:55:10,240
If you're were a woman--

917
00:55:10,240 --> 00:55:13,840
has anybody visited the Grace
Murray Hopper Conference?

918
00:55:13,840 --> 00:55:15,900
A couple, a few of you.

919
00:55:15,900 --> 00:55:17,640
So this is a really
cool conference.

920
00:55:17,640 --> 00:55:22,350
Grace Murray Hopper invented
the notion bug or the term bug

921
00:55:22,350 --> 00:55:25,650
and was a really famous
computer scientist starting back

922
00:55:25,650 --> 00:55:29,170
in the 1940s when there
were very few of them,

923
00:55:29,170 --> 00:55:31,980
and there is a yearly
conference for women computer

924
00:55:31,980 --> 00:55:34,500
scientists in her honor.

925
00:55:34,500 --> 00:55:39,210
So clearly, the probability that
you visited the Grace Hopper

926
00:55:39,210 --> 00:55:42,360
Conference is dependent
on your gender.

927
00:55:42,360 --> 00:55:45,588
It's also dependent on whether
you're a computer scientist,

928
00:55:45,588 --> 00:55:47,130
because if you're
a historian, you're

929
00:55:47,130 --> 00:55:51,870
not likely to be interested
in going to that conference.

930
00:55:51,870 --> 00:55:56,490
And so in this story,
the optimal score

931
00:55:56,490 --> 00:55:59,850
is going to depend basically
on whether you have a computer

932
00:55:59,850 --> 00:56:05,960
science degree or not,
but the separated score

933
00:56:05,960 --> 00:56:09,020
will depend only on
your gender, which

934
00:56:09,020 --> 00:56:12,590
is kind of funny, because
that's the protected attribute.

935
00:56:12,590 --> 00:56:17,510
And what these guys point out is
that despite the fact that you

936
00:56:17,510 --> 00:56:21,500
have these two scenarios,
it could well turn out

937
00:56:21,500 --> 00:56:25,610
that the numerical data, the
statistics from which you

938
00:56:25,610 --> 00:56:29,850
estimate these models
are absolutely identical.

939
00:56:29,850 --> 00:56:32,330
In other words, the
same fraction of people

940
00:56:32,330 --> 00:56:37,220
are men and women, the
same fraction of people

941
00:56:37,220 --> 00:56:40,250
are programmers, they
have the same relationship

942
00:56:40,250 --> 00:56:44,780
to those other factors, and
so from a purely observational

943
00:56:44,780 --> 00:56:51,250
viewpoint, you can't tell
which of these styles of model

944
00:56:51,250 --> 00:57:00,640
is correct or which version of
fairness your data can support.

945
00:57:00,640 --> 00:57:03,160
So that's a problem
because we know

946
00:57:03,160 --> 00:57:06,760
that these different
notions of fairness

947
00:57:06,760 --> 00:57:09,170
are in conflict with each other.

948
00:57:09,170 --> 00:57:13,190
So I wanted to finish by showing
you a couple of examples.

949
00:57:13,190 --> 00:57:17,890
So this was a paper
based on Irene's work.

950
00:57:17,890 --> 00:57:25,720
So Irene, shout if I'm
butchering the discussion.

951
00:57:25,720 --> 00:57:33,790
I got an invitation last year
from the American Medical

952
00:57:33,790 --> 00:57:39,040
Association's Journal of Ethics,
which I didn't know existed,

953
00:57:39,040 --> 00:57:42,940
to write a think piece for
them about fairness in machine

954
00:57:42,940 --> 00:57:48,340
learning, and I decided that
rather than just bloviate,

955
00:57:48,340 --> 00:57:50,380
I wanted to present
some real work,

956
00:57:50,380 --> 00:57:53,920
and Irene had been
doing some real work.

957
00:57:53,920 --> 00:57:56,530
And so Marcia, who was
one of my students,

958
00:57:56,530 --> 00:58:00,890
and I convinced her
to get into this,

959
00:58:00,890 --> 00:58:05,720
and we started looking
at the question of how

960
00:58:05,720 --> 00:58:09,800
these machine learning models
can identify and perhaps reduce

961
00:58:09,800 --> 00:58:15,110
disparities in general
medical and mental health.

962
00:58:15,110 --> 00:58:16,730
Now, why those two areas?

963
00:58:16,730 --> 00:58:19,740
Because we had access
to data in those areas.

964
00:58:19,740 --> 00:58:23,000
So the general medical was
actually not that general.

965
00:58:23,000 --> 00:58:25,490
It's intensive care
data from MIMIC,

966
00:58:25,490 --> 00:58:27,470
and mental health
care is some data

967
00:58:27,470 --> 00:58:32,120
that we had access to from Mass
General and McLean's hospital

968
00:58:32,120 --> 00:58:35,195
here in Boston, which both
have big psychiatric clinics.

969
00:58:35,195 --> 00:58:37,830

970
00:58:37,830 --> 00:58:42,580
So yeah, this is
what I just said.

971
00:58:42,580 --> 00:58:44,910
So the question we were
asking is, is there

972
00:58:44,910 --> 00:58:48,570
bias based on race,
gender, and insurance type?

973
00:58:48,570 --> 00:58:51,700
So we were really interested
in socioeconomic status,

974
00:58:51,700 --> 00:58:54,090
but we didn't have
that in the database,

975
00:58:54,090 --> 00:58:57,900
but the type of insurance you
have correlates pretty well

976
00:58:57,900 --> 00:59:00,180
with whether you're
rich or poor.

977
00:59:00,180 --> 00:59:03,780
If you have Medicaid insurance,
for example, you're poor,

978
00:59:03,780 --> 00:59:05,590
and if you have
private insurance,

979
00:59:05,590 --> 00:59:08,490
the first approximation,
you're rich.

980
00:59:08,490 --> 00:59:12,120
So we did that, and then
we looked at the notes.

981
00:59:12,120 --> 00:59:15,330
So we wanted to see
not the coded data,

982
00:59:15,330 --> 00:59:19,050
but whether the things that
nurses and doctors said

983
00:59:19,050 --> 00:59:22,650
about you as you
were in the hospital

984
00:59:22,650 --> 00:59:27,210
were predictive of readmission,
of 30-day readmission,

985
00:59:27,210 --> 00:59:30,850
of whether you were likely
to come back to the hospital.

986
00:59:30,850 --> 00:59:32,900
So these are some of the topics.

987
00:59:32,900 --> 00:59:38,220
We used LDA, standard
topic modeling framework.

988
00:59:38,220 --> 00:59:41,790
And the topics, as usual,
include some garbage,

989
00:59:41,790 --> 00:59:46,750
but also include a lot of
recognizably useful topics.

990
00:59:46,750 --> 00:59:49,950
So for example, mass,
cancer, metastatic,

991
00:59:49,950 --> 00:59:54,090
clearly associated with
cancer, Afib, atrial, Coumadin,

992
00:59:54,090 --> 00:59:59,850
fibrillation, associated with
heart function, et cetera,

993
00:59:59,850 --> 01:00:01,620
in the ICU domain.

994
01:00:01,620 --> 01:00:04,230
In the psychiatric
domain, you have

995
01:00:04,230 --> 01:00:08,280
things like bipolar, lithium,
manic episode, clearly

996
01:00:08,280 --> 01:00:13,620
associated with bipolar disease,
pain, chronic, milligrams,

997
01:00:13,620 --> 01:00:18,390
the drug quantity, associated
with chronic pain, et cetera.

998
01:00:18,390 --> 01:00:22,970
So these were the
topics that we used.

999
01:00:22,970 --> 01:00:26,360
And so we said,
what happens when

1000
01:00:26,360 --> 01:00:32,180
you look at the
different topics,

1001
01:00:32,180 --> 01:00:34,400
how often the
different topics arise

1002
01:00:34,400 --> 01:00:36,860
in different subpopulations?

1003
01:00:36,860 --> 01:00:41,330
And so what we found is that,
for example, white patients

1004
01:00:41,330 --> 01:00:44,780
have more topics that
are enriched for anxiety

1005
01:00:44,780 --> 01:00:49,760
and chronic pain, whereas black,
Hispanic, and Asian patients

1006
01:00:49,760 --> 01:00:52,430
had higher topic
enrichment for psychosis.

1007
01:00:52,430 --> 01:00:55,460

1008
01:00:55,460 --> 01:00:57,500
It's interesting.

1009
01:00:57,500 --> 01:01:00,890
Male patients had more
substance abuse problems.

1010
01:01:00,890 --> 01:01:04,010
Female patients had
more general depression

1011
01:01:04,010 --> 01:01:06,650
and treatment-resistant
depression.

1012
01:01:06,650 --> 01:01:13,040
So if you want to create a
stereotype, men are druggies

1013
01:01:13,040 --> 01:01:18,530
and women are depressed,
according to this data.

1014
01:01:18,530 --> 01:01:19,860
What about insurance type?

1015
01:01:19,860 --> 01:01:24,590
Well, private insurance
patients had higher levels

1016
01:01:24,590 --> 01:01:28,310
of anxiety and depression,
and poorer patients

1017
01:01:28,310 --> 01:01:31,310
or public insurance
patients had more problems

1018
01:01:31,310 --> 01:01:33,440
with substance abuse.

1019
01:01:33,440 --> 01:01:37,070
Again, another stereotype
that you could form.

1020
01:01:37,070 --> 01:01:41,900
And then you could look at--

1021
01:01:41,900 --> 01:01:44,420
that was in the
psychiatric population.

1022
01:01:44,420 --> 01:01:51,770
In the ICU population, men still
have substance abuse problems.

1023
01:01:51,770 --> 01:01:55,880
Women have more
pulmonary disease.

1024
01:01:55,880 --> 01:01:57,830
And we were
speculating on how this

1025
01:01:57,830 --> 01:02:01,760
relates to sort of known
data about underdiagnosis

1026
01:02:01,760 --> 01:02:04,400
of COPD in women.

1027
01:02:04,400 --> 01:02:09,920
By race, Asian patients have
a lot of discussion of cancer,

1028
01:02:09,920 --> 01:02:11,930
black patients have
a lot of discussion

1029
01:02:11,930 --> 01:02:15,800
of kidney problems,
Hispanics of liver problems,

1030
01:02:15,800 --> 01:02:18,630
and whites have
atrial fibrillation.

1031
01:02:18,630 --> 01:02:22,310
So again, stereotypes
of what's most common

1032
01:02:22,310 --> 01:02:23,560
in these different groups.

1033
01:02:23,560 --> 01:02:29,780

1034
01:02:29,780 --> 01:02:33,350
And by insurance type,
those with public insurance

1035
01:02:33,350 --> 01:02:36,530
often have multiple
chronic conditions.

1036
01:02:36,530 --> 01:02:41,060
And so public insurance patients
have atrial fibrillation,

1037
01:02:41,060 --> 01:02:43,130
pacemakers, dialysis.

1038
01:02:43,130 --> 01:02:48,950
These are indications
of chronic heart disease

1039
01:02:48,950 --> 01:02:51,080
and chronic kidney disease.

1040
01:02:51,080 --> 01:02:54,560
And private insurance patients
have higher topic enrichment

1041
01:02:54,560 --> 01:02:56,810
values for fractures.

1042
01:02:56,810 --> 01:02:59,420
So maybe they're richer,
they play more sports

1043
01:02:59,420 --> 01:03:02,300
and break their
arms or something.

1044
01:03:02,300 --> 01:03:04,300
Lymphoma and aneurysms.

1045
01:03:04,300 --> 01:03:06,930

1046
01:03:06,930 --> 01:03:08,610
Just reporting the data.

1047
01:03:08,610 --> 01:03:10,210
Just the facts.

1048
01:03:10,210 --> 01:03:13,470
So these results are
actually consistent with lots

1049
01:03:13,470 --> 01:03:17,880
of analysis that have been
done of this kind of data.

1050
01:03:17,880 --> 01:03:20,160
Now, what I really
wanted to look at

1051
01:03:20,160 --> 01:03:23,640
was this question of, can
we get similar error rates,

1052
01:03:23,640 --> 01:03:26,910
or how similar are the
error rates that we get,

1053
01:03:26,910 --> 01:03:29,610
and the answer is, not so much.

1054
01:03:29,610 --> 01:03:34,710
So for example, if you
look at the ICU data,

1055
01:03:34,710 --> 01:03:39,830
we find that the error rates
on a zero-one loss metric

1056
01:03:39,830 --> 01:03:45,100
are much lower for men than they
are for women, statistically

1057
01:03:45,100 --> 01:03:47,240
significantly lower.

1058
01:03:47,240 --> 01:03:52,900
So we're able to more accurately
model male response or male

1059
01:03:52,900 --> 01:03:57,100
prediction of 30-day
readmission than we are--

1060
01:03:57,100 --> 01:04:02,350
sorry, of ICU mortality for
the ICU than we are for women.

1061
01:04:02,350 --> 01:04:08,680
Similarly, we have
much tighter ability

1062
01:04:08,680 --> 01:04:13,090
to predict outcomes for
private insurance patients

1063
01:04:13,090 --> 01:04:15,910
than for public
insurance patients

1064
01:04:15,910 --> 01:04:19,210
with a huge gap
in the confidence

1065
01:04:19,210 --> 01:04:21,290
intervals between them.

1066
01:04:21,290 --> 01:04:24,250
So this indicates that
there is, in fact,

1067
01:04:24,250 --> 01:04:27,640
a racial bias in the
data that we have

1068
01:04:27,640 --> 01:04:30,160
and in the models
that we're building.

1069
01:04:30,160 --> 01:04:33,580
These are particularly
simple models.

1070
01:04:33,580 --> 01:04:39,250
In psychiatry, when you
look at the comparison

1071
01:04:39,250 --> 01:04:41,680
for different
ethnic populations,

1072
01:04:41,680 --> 01:04:44,560
you see a fair
amount of overlap.

1073
01:04:44,560 --> 01:04:47,170
One reason we
speculate is that we

1074
01:04:47,170 --> 01:04:50,770
have a lot less data
about psychiatric patients

1075
01:04:50,770 --> 01:04:53,270
than we do about ICU patients.

1076
01:04:53,270 --> 01:04:54,910
So the models are
not going to give us

1077
01:04:54,910 --> 01:04:56,860
as accurate predictions.

1078
01:04:56,860 --> 01:05:02,320
But you still see, for example,
a statistically significant

1079
01:05:02,320 --> 01:05:10,530
difference between blacks
and whites and other races,

1080
01:05:10,530 --> 01:05:13,970
although there's a
lot of overlap here.

1081
01:05:13,970 --> 01:05:16,400
Again, between
males and females,

1082
01:05:16,400 --> 01:05:20,820
we get fewer errors in
making predictions for males,

1083
01:05:20,820 --> 01:05:26,120
but there is not a 95%
confidence separation

1084
01:05:26,120 --> 01:05:27,420
between them.

1085
01:05:27,420 --> 01:05:30,350
And for private versus
public insurance,

1086
01:05:30,350 --> 01:05:33,810
we do see that separation
where for some reason,

1087
01:05:33,810 --> 01:05:36,470
in fact, we're able to
make better predictions

1088
01:05:36,470 --> 01:05:38,240
for the people on
Medicare than we

1089
01:05:38,240 --> 01:05:41,270
are-- or Medicaid than
we are for patients

1090
01:05:41,270 --> 01:05:43,830
in private insurance.

1091
01:05:43,830 --> 01:05:49,040
So just to wrap that up, this is
not a solution to the problem,

1092
01:05:49,040 --> 01:05:53,120
but it's an examination
of the problem.

1093
01:05:53,120 --> 01:05:55,940
And this Journal of
Ethics considered

1094
01:05:55,940 --> 01:06:01,340
it interesting enough to publish
just a couple of months ago.

1095
01:06:01,340 --> 01:06:03,560
The last thing I
want to talk about

1096
01:06:03,560 --> 01:06:07,100
is some work of
Willie's, so I'm taking

1097
01:06:07,100 --> 01:06:09,950
the risk of speaking
before the people who

1098
01:06:09,950 --> 01:06:14,960
actually did the work here
and embarrassing myself.

1099
01:06:14,960 --> 01:06:18,470
So this is modeling mistrust
in end-of-life care,

1100
01:06:18,470 --> 01:06:22,190
and it's based on
Willie's master's thesis

1101
01:06:22,190 --> 01:06:25,280
and on some papers that
came as a result of that.

1102
01:06:25,280 --> 01:06:27,800

1103
01:06:27,800 --> 01:06:32,960
So here's the interesting data.

1104
01:06:32,960 --> 01:06:35,990
If you look at
African-American patients,

1105
01:06:35,990 --> 01:06:42,260
and these are patients in the
MIMIC data set, what you find

1106
01:06:42,260 --> 01:06:47,720
is that for mechanical
ventilation,

1107
01:06:47,720 --> 01:06:50,180
blacks are on
mechanical ventilation

1108
01:06:50,180 --> 01:06:54,110
a lot longer than
whites on average,

1109
01:06:54,110 --> 01:06:56,660
and there's a pretty
decent separation

1110
01:06:56,660 --> 01:07:00,230
at the P equal
0.05 level, so 1/2%

1111
01:07:00,230 --> 01:07:03,330
level between those
two populations.

1112
01:07:03,330 --> 01:07:07,220
So there's something going
on where black patients are

1113
01:07:07,220 --> 01:07:11,480
kept on mechanical ventilation
longer than white patients.

1114
01:07:11,480 --> 01:07:14,270
Now, of course, we
don't know exactly why.

1115
01:07:14,270 --> 01:07:16,100
We don't know whether
it's because there

1116
01:07:16,100 --> 01:07:19,250
is a physiological
difference, or because it

1117
01:07:19,250 --> 01:07:21,320
has something to do
with their insurance,

1118
01:07:21,320 --> 01:07:23,120
or because God knows.

1119
01:07:23,120 --> 01:07:26,100
It could be any of a lot
of different factors,

1120
01:07:26,100 --> 01:07:27,770
but that's the case.

1121
01:07:27,770 --> 01:07:30,390
The eICU data set
we've mentioned,

1122
01:07:30,390 --> 01:07:34,430
it's a larger, but less
detailed data set, also of

1123
01:07:34,430 --> 01:07:39,380
intensive care patients, that
was donated to Roger Marks' Lab

1124
01:07:39,380 --> 01:07:41,960
by Phillips Corporation.

1125
01:07:41,960 --> 01:07:45,410
And there, we see,
again, a separation

1126
01:07:45,410 --> 01:07:49,430
of mechanical ventilation
duration roughly

1127
01:07:49,430 --> 01:07:52,250
comparable to what we saw
in the MIMIC data set.

1128
01:07:52,250 --> 01:07:55,380
So these are consistent
with each other.

1129
01:07:55,380 --> 01:07:59,450
On the other hand, if you look
at the use of vasopressors,

1130
01:07:59,450 --> 01:08:04,250
blacks versus whites, at
the P equal 0.12 level,

1131
01:08:04,250 --> 01:08:05,930
you say, well,
there's a little bit

1132
01:08:05,930 --> 01:08:07,730
of evidence, but
not strong enough

1133
01:08:07,730 --> 01:08:09,650
to reach any conclusions.

1134
01:08:09,650 --> 01:08:14,480
Or in the eICU
data, P equal 0.42

1135
01:08:14,480 --> 01:08:17,899
is clearly quite
insignificant, so we're not

1136
01:08:17,899 --> 01:08:20,029
making any claims there.

1137
01:08:20,029 --> 01:08:22,729
So the question that
Willie was asking,

1138
01:08:22,729 --> 01:08:27,300
which I think is a
really good question, is,

1139
01:08:27,300 --> 01:08:33,410
could this difference be due
not to physiological differences

1140
01:08:33,410 --> 01:08:37,069
or even these sort of
socioeconomic or social

1141
01:08:37,069 --> 01:08:41,300
differences, but to a difference
in the degree of trust

1142
01:08:41,300 --> 01:08:45,500
between the patient
and their doctors?

1143
01:08:45,500 --> 01:08:48,010
It's an interesting idea.

1144
01:08:48,010 --> 01:08:51,640
And of course, I wouldn't
be telling you about this

1145
01:08:51,640 --> 01:08:54,580
if the answer were no.

1146
01:08:54,580 --> 01:08:58,300
And so the approach
that he took was

1147
01:08:58,300 --> 01:09:02,260
to look for cases where
there's clearly mistrust.

1148
01:09:02,260 --> 01:09:06,580
So there are red flags
if you read the notes.

1149
01:09:06,580 --> 01:09:09,670
For example, if a patient
leaves the hospital

1150
01:09:09,670 --> 01:09:14,170
against medical advice, that
is a pretty good indication

1151
01:09:14,170 --> 01:09:17,979
that they don't trust
the medical system.

1152
01:09:17,979 --> 01:09:22,390
If the family-- if the
person dies and the family

1153
01:09:22,390 --> 01:09:26,240
refuses to allow them
to do an autopsy,

1154
01:09:26,240 --> 01:09:28,689
this is another
indication that maybe they

1155
01:09:28,689 --> 01:09:30,800
don't trust the medical system.

1156
01:09:30,800 --> 01:09:36,010
So there are these sort of red
letter indicators of mistrust.

1157
01:09:36,010 --> 01:09:40,420
For example, patient
refused to sign ICU consent

1158
01:09:40,420 --> 01:09:42,790
and expressed
wishes to be do not

1159
01:09:42,790 --> 01:09:45,880
resuscitate, do not
intubate, seemingly very

1160
01:09:45,880 --> 01:09:49,479
frustrated and mistrusting
of the health care system,

1161
01:09:49,479 --> 01:09:52,870
also with a history of
poor medication compliance

1162
01:09:52,870 --> 01:09:54,090
and follow-up.

1163
01:09:54,090 --> 01:09:56,080
So that's a pretty
clear indication.

1164
01:09:56,080 --> 01:10:00,910
And you can build a
relatively simple extraction

1165
01:10:00,910 --> 01:10:06,590
or interpretation model that
identifies those clear cases.

1166
01:10:06,590 --> 01:10:10,190
This is what I was
saying about autopsies.

1167
01:10:10,190 --> 01:10:12,890
So the problem, of course,
is that not every patient

1168
01:10:12,890 --> 01:10:14,990
has such an obvious label.

1169
01:10:14,990 --> 01:10:17,130
In fact, most of them don't.

1170
01:10:17,130 --> 01:10:21,500
And so Willie's idea
was, can we learn a model

1171
01:10:21,500 --> 01:10:24,530
from these obvious
examples and then

1172
01:10:24,530 --> 01:10:27,680
apply them to the
less obvious examples

1173
01:10:27,680 --> 01:10:31,430
in order to get a kind
of a bronze standard

1174
01:10:31,430 --> 01:10:37,250
or remote supervision notion
of a larger population that

1175
01:10:37,250 --> 01:10:42,020
has a tendency to be mistrustful
according to our model

1176
01:10:42,020 --> 01:10:46,850
without having as explicit
a clear case of mistrust,

1177
01:10:46,850 --> 01:10:49,810
as in those examples.

1178
01:10:49,810 --> 01:10:55,300
And so if you look at chart
events in MIMIC, for example,

1179
01:10:55,300 --> 01:10:59,380
you discover that
associated with those cases

1180
01:10:59,380 --> 01:11:04,480
of obvious mistrust are
features like the person was

1181
01:11:04,480 --> 01:11:06,370
in restraints.

1182
01:11:06,370 --> 01:11:09,040
They were literally
locked down to their bed

1183
01:11:09,040 --> 01:11:11,890
because the nurses
were afraid they would

1184
01:11:11,890 --> 01:11:15,280
get up and do something bad.

1185
01:11:15,280 --> 01:11:17,710
Not necessarily
like attack a nurse,

1186
01:11:17,710 --> 01:11:22,810
but more like fall out of bed
or go wandering off the floor

1187
01:11:22,810 --> 01:11:25,180
or something like that.

1188
01:11:25,180 --> 01:11:29,920
If a person is in pain, that
correlated with these mistrust

1189
01:11:29,920 --> 01:11:31,480
measures as well.

1190
01:11:31,480 --> 01:11:36,280
And conversely, if you saw that
somebody had their hair washed

1191
01:11:36,280 --> 01:11:40,130
or that there was a discussion
of their status and comfort,

1192
01:11:40,130 --> 01:11:43,420
then they were probably
less likely to be

1193
01:11:43,420 --> 01:11:46,460
mistrustful of the system.

1194
01:11:46,460 --> 01:11:49,120
And so the approach
that Willie took

1195
01:11:49,120 --> 01:11:54,100
was to say, well, let's code
these 620 binary indicators

1196
01:11:54,100 --> 01:11:57,370
of trust and build a
logistic regression

1197
01:11:57,370 --> 01:12:00,940
model to the labeled
examples and then

1198
01:12:00,940 --> 01:12:04,420
apply it to the unlabeled
examples of people for whom

1199
01:12:04,420 --> 01:12:07,540
we don't have such
a clear indication,

1200
01:12:07,540 --> 01:12:11,380
and this gives us another
population of people who

1201
01:12:11,380 --> 01:12:14,410
are likely to be
mistrustful and therefore,

1202
01:12:14,410 --> 01:12:19,260
enough people that we can
do further analysis on it.

1203
01:12:19,260 --> 01:12:22,730
So if you look at
the mistrust metrics,

1204
01:12:22,730 --> 01:12:27,050
you have things like if
the patient is agitated

1205
01:12:27,050 --> 01:12:32,690
on some agitation scale, they're
more likely to be mistrustful.

1206
01:12:32,690 --> 01:12:35,180
If, conversely,
they're alert, they're

1207
01:12:35,180 --> 01:12:37,600
less likely to be mistrustful.

1208
01:12:37,600 --> 01:12:40,340
So that means they're in
some better mental shape.

1209
01:12:40,340 --> 01:12:42,290
If they're not in
pain, they're less

1210
01:12:42,290 --> 01:12:46,050
likely to be
mistrustful, et cetera.

1211
01:12:46,050 --> 01:12:51,860
And if the patient
was restrained,

1212
01:12:51,860 --> 01:12:56,780
then trustful
patients have no pain,

1213
01:12:56,780 --> 01:13:01,130
or they have a spokesperson
who is their health care proxy,

1214
01:13:01,130 --> 01:13:03,860
or there is a lot of
family communication,

1215
01:13:03,860 --> 01:13:09,770
but conversely, if restraints
had to be reapplied,

1216
01:13:09,770 --> 01:13:15,710
or if there are various
other factors, then

1217
01:13:15,710 --> 01:13:19,390
they're more likely
to be mistrustful.

1218
01:13:19,390 --> 01:13:27,010
So if you look at that
prediction, what you find

1219
01:13:27,010 --> 01:13:30,670
is that for both predicting the
use of mechanical ventilation

1220
01:13:30,670 --> 01:13:36,040
and vasopressors, the
disparity between a population

1221
01:13:36,040 --> 01:13:39,730
of black and white
patients is actually

1222
01:13:39,730 --> 01:13:43,480
less significant
than the disparity

1223
01:13:43,480 --> 01:13:48,920
between a population of high
trust and low trust patients.

1224
01:13:48,920 --> 01:13:53,260
So what this suggests is that
the fundamental feature here

1225
01:13:53,260 --> 01:13:55,480
that may be leading
to that difference

1226
01:13:55,480 --> 01:13:58,780
is, in fact, not
race, but is something

1227
01:13:58,780 --> 01:14:01,840
that correlates with
race because blacks

1228
01:14:01,840 --> 01:14:03,850
are more likely
to be distrustful

1229
01:14:03,850 --> 01:14:06,070
of the medical
system than whites.

1230
01:14:06,070 --> 01:14:07,808
Now, why might that be?

1231
01:14:07,808 --> 01:14:09,100
What do you know about history?

1232
01:14:09,100 --> 01:14:12,760

1233
01:14:12,760 --> 01:14:15,580
I mean, you took the
city training course

1234
01:14:15,580 --> 01:14:19,660
that had you read the Belmont
Report talking about things

1235
01:14:19,660 --> 01:14:22,570
like the Tuskegee experiment.

1236
01:14:22,570 --> 01:14:26,410
I'm sure that leaves a
significant impression

1237
01:14:26,410 --> 01:14:30,100
in people's minds about how
the health care system is going

1238
01:14:30,100 --> 01:14:33,660
to treat people of their race.

1239
01:14:33,660 --> 01:14:34,670
I'm Jewish.

1240
01:14:34,670 --> 01:14:39,070
My mother barely lived
through Auschwitz,

1241
01:14:39,070 --> 01:14:42,940
and so I understand some
of the strong family

1242
01:14:42,940 --> 01:14:45,760
feelings that happened
as a result of some

1243
01:14:45,760 --> 01:14:47,740
of these historical events.

1244
01:14:47,740 --> 01:14:52,600
And there were medical people
doing experiments on prisoners

1245
01:14:52,600 --> 01:14:55,370
in the concentration
camps as well,

1246
01:14:55,370 --> 01:14:58,390
so I would expect that
people in my status

1247
01:14:58,390 --> 01:15:03,860
might also have similar
issues of mistrust.

1248
01:15:03,860 --> 01:15:06,420
Now, it turns out,
you might ask, well,

1249
01:15:06,420 --> 01:15:10,220
is mistrust, in fact,
just a proxy for severity?

1250
01:15:10,220 --> 01:15:13,490
Are sicker people
simply more mistrustful,

1251
01:15:13,490 --> 01:15:17,390
and is what we're seeing
just a reflection of the fact

1252
01:15:17,390 --> 01:15:18,740
that they're sicker?

1253
01:15:18,740 --> 01:15:21,030
And the answer seems
to be, not so much.

1254
01:15:21,030 --> 01:15:27,620
So if you look at these severity
scores like OASIS and SAPS

1255
01:15:27,620 --> 01:15:32,240
and look at their correlation
with noncompliance in autopsy,

1256
01:15:32,240 --> 01:15:34,370
those are pretty low
correlation values,

1257
01:15:34,370 --> 01:15:39,360
so they're not explanatory
of this phenomenon.

1258
01:15:39,360 --> 01:15:43,440
And then in the population,
you see that, again, there

1259
01:15:43,440 --> 01:15:48,230
is a significant
difference in sentiment

1260
01:15:48,230 --> 01:15:56,330
expressed in the notes between
black and white patients.

1261
01:15:56,330 --> 01:16:00,560
The autopsy derived
mistrust metrics don't

1262
01:16:00,560 --> 01:16:05,070
show a strong relationship, a
strong difference between them,

1263
01:16:05,070 --> 01:16:11,340
but the noncompliance
derived mistrust metrics do.

1264
01:16:11,340 --> 01:16:14,140
So I'm out of time.

1265
01:16:14,140 --> 01:16:17,390
I'll just leave you
with a final word.

1266
01:16:17,390 --> 01:16:21,690
There is a lot more work that
needs to be done in this area,

1267
01:16:21,690 --> 01:16:27,180
and it's a very rich area
both for technical work

1268
01:16:27,180 --> 01:16:30,930
and for trying to understand
what the desiderata are

1269
01:16:30,930 --> 01:16:34,500
and how to match them to
the technical capabilities.

1270
01:16:34,500 --> 01:16:37,860
There are these
various conferences.

1271
01:16:37,860 --> 01:16:41,100
One of the people
active in this area, one

1272
01:16:41,100 --> 01:16:45,960
of the pairs of people, Mike
Kearns and Aaron Roth at Penn

1273
01:16:45,960 --> 01:16:49,180
are coming out with a book
called The Ethical Algorithm,

1274
01:16:49,180 --> 01:16:51,510
which is coming out this fall.

1275
01:16:51,510 --> 01:16:52,920
It's a popular pressbook.

1276
01:16:52,920 --> 01:16:55,650
I've not read it,
but it looks like it

1277
01:16:55,650 --> 01:16:57,750
should be quite interesting.

1278
01:16:57,750 --> 01:17:00,720
And then we're starting
to see whole classes

1279
01:17:00,720 --> 01:17:04,260
in fairness popping up at
different universities.

1280
01:17:04,260 --> 01:17:08,460
University of Pennsylvania has
the science of Data ethics,

1281
01:17:08,460 --> 01:17:11,430
and I've mentioned already this
fairness in machine learning

1282
01:17:11,430 --> 01:17:13,510
class at Berkeley.

1283
01:17:13,510 --> 01:17:16,930
This is, in fact, one of the
topics we've talked about.

1284
01:17:16,930 --> 01:17:19,260
I'm on a committee
that is planning

1285
01:17:19,260 --> 01:17:21,120
the activities of
the new Schwarzman

1286
01:17:21,120 --> 01:17:24,510
College of Computing,
and this notion

1287
01:17:24,510 --> 01:17:27,990
of infusing ideas about
fairness and ethics

1288
01:17:27,990 --> 01:17:31,230
into the technical curriculum
is one of the things

1289
01:17:31,230 --> 01:17:32,920
that we've been discussing.

1290
01:17:32,920 --> 01:17:35,020
The college obviously
hasn't started yet,

1291
01:17:35,020 --> 01:17:38,700
so we don't have anything
other than this lecture

1292
01:17:38,700 --> 01:17:41,280
and a few other things
like that in the works,

1293
01:17:41,280 --> 01:17:45,950
but the plan is there to
expand more in this area.

1294
01:17:45,950 --> 01:17:55,000

